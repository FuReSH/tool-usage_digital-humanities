

# Introduction 


Smells are an important, yet overlooked part of cultural heritage (Bembibre and Strlič, 2017). TheOdeuropa project analyzes large amounts of visual and textual corpora toinvestigate the cultural dimensions of smell in 16th – 20th century Europe.The study of pictorial representations bears a specific challenge: the substrate of smell is usually invisible (Marx, 2021). Object detection is a well-researched computer vision technique, and sowe start with the recognition of objects, which may then serve as a basisfor the indirect recognition of more complex, and possibly more meaningful,smell references such as gestures, spaces, or iconographic allusions (Zinnen, 2021). 



# Figure 1: 


Smell. The Five Senses. 1558 – 1617. Jan Pietersz Saenredam. National Gallery of Art. 
However, the detection of olfactory objects in historical artworks is achallenging task. The visual representation of objects differs significantlybetween artworks and photographs (Hall et al., 2015). Since state-of-the-art object detectionalgorithms are trained and evaluated on large-scale photographic datasetssuch as ImageNet (Russakovsky et al., 2015), MS COCO (Lin et al., 2014), or OpenImages (Kuznetsova et al., 2020), their performancedrops significantly when applied to artistic data. This domain gap betweenstandard object detection datasets and artistic imagery can be mitigatedby training directly on artworks, either by using existing datasets or bycreating an annotated dataset for the target domain. Another challengeis the mismatch between object categories present in modern datasets andhistorical olfactory objects, caused by historical diachrony on the one hand (Marinescu et al., 2020), and the particularity of some smell-relevant objects on the other (Ehrich et al., 2021). 


# Methodology 


To overcome the domain gap and category mismatch between our applicationand the existing datasets, we apply transfer learning – a training strategy where machine learning algorithms are pre-trained in one domain and then fine-tuned in another, greatly decreasing the amount of required training data in the target domain (Sinno and Yang, 2009; Zhuan et al., 2020, Madhu et al., 2020). We are continuously collecting and annotating artworks with possible olfactory relevance from multiple museum collections. Based on these, we created a dataset of olfactory artworks containing 16728 annotations on 2229 artworks. From this full set of annotations, we created a test set of 3416 annotations on 473 artworks, while the remaining data was used for training. 



# Figure 2: 


Category overlap between Odeuropa & OpenImages categories 
A common transfer learning procedure is to use detection backbones that have been pre-trained on ImageNet and fine-tune them for object detection (Zhuang et al., 2020). We expand this strategy by an additional pre-training step, where we train an ImageNet pre-trained object detection network (Ren et al., 2015) using different datasets. Finally, we fine-tune the resulting model using our olfactory artworks dataset (fig. 3). 

For pre-training, we use three 

# Figure 3: 


Category overlap between Odeuropa & OpenImages categories 
different datasets, deviating to varying amounts from our olfactory artworks dataset in terms of categories and style (table 1): a) Same Categories, Different Styles - A subset of OpenImages (OI) containing only odor objects results in a complete category match (fig. 2); however, since OpenImages contains only photographs, there is a considerable style difference. b) Different Categories, Same Styles - We apply two object detection datasets from the art domain, which are more similar in terms of style but contain different object categories, namely IconArt (IA,Gonthier et al., 2018) and PeopleArt (PA,Westlake et al., 2016). 



# Table 2: Evaluation of object detection performance. The best performing model pre-trained with OI achieves an improvement of 6.5% pascal VOC mAP, and 3.4% COCO mAP over the baseline method without intermediate training. We report the evaluation for each pre-training dataset, averaged over five models, fine-tuned for 50 epochs on our olfactory artworks datasets. Best evaluation results are highlighted in bold. The merge of two datasets D1 and D2 is written as D1 ∪ D2. 

| Dataset  || Domain Similarity  || Category Similarity  || # Categories  || OpenImages (OI)  || Low  || Complete match  || 29  || IconArt (IA)  || High  || Medium  || 10  || PeopleArt (PA)  || Medium  || Low  || 1  |



# Results 


To ensure a fair comparison between the different pre-training datasets, we reduce each of the datasets to the same size, train three models, and select the best according to a fixed validation set for each dataset. Additionally, we merge all three datasets, i. e., combining OI, IA, and PA, using the union over their respective classes. The resulting models are then fine-tuned on the 

training set of the olfactory artworks dataset and evaluated on a separate test set. To mitigate random variations that can occur during the training process, we train five separate models for each experimental setting and report their average. Evaluation results are reported in pascal VOC (mAP 50, Everingham et al., 2010) and COCO mAP (mAP 50:95:5 (Lin et al., 2014), the two standard metrics to evaluate object detection models. We conduct two separate sets of experiments: In the first, we fine-tune the whole network, including the backbone, to assess the detection performance under realistic conditions (table 2). 



# Table 3: Evaluation of object detection performance for fine-tuning of thedetection heads only. All pre-training schemes increase the detection perfor-mance, while pre-training with OI leads to the best results with an increaseof 7.7% mAP 50 or 4% COCO mAP. For every pre-training dataset, we report the evaluation averaged over five models, fine-tuned for 50 epochs onour olfactory artworks datasets each. Best evaluation results are marked in bold. The merge of two datasets D1 and D2 is written as D1 ∪ D2. 

| Pretraining Dataset  || Pascal mAP (%)  || COCO mAP (%)  || None (Baseline)  || 16.8 (±1.3)  || 8.4 (±0.4)  || OI  || 23.3 (±0.5)  || 11 .8 (±0.4)  || IA  || 22.6 (±1.2)  || 10.9 (±0.9)  || PA  || 21.9 (±0.4)  || 10.5 (±0.2)  || IA ∪ OI  || 21.8 (±0.1)  || 10.5 (±0.3)  || IA ∪ PA  || 22.0 (±0.8)  || 10.6 (±0.3)  || PA ∪ OI  || 22.6 (±0.3)  || 10.8 (±0.2)  || OI ∪ IA ∪ PA  || 21.8 (±0.4)  || 10.5 (±0.2)  |




# Figure 4: 


Exemplary object predictions for a detection model without intermediate training (left), with PeopleArt pretraining (middle), and ground truth bounding boxes (right). Painting: Boy holding a pewter tankard, by a still life of a duck, cheeses, bread and a herring. 1625 – 1674. Gerard van Honthorst. RKD Digital Collection (https://rkd.nl/explore/images/287165). 
We observe a performance increase for all used pre-training datasets, with an increase of 6.5%/3,4% boost in mAP 50 and COCO mAP, respectively, for the best performing pre-training scheme, which was achieved using the OI dataset. The exemplary object predictions in fig. 4 show that adding an additional pre-training stage can increase the number of recognized objects. In a second set of experiments, we train only the detection head while the backbone remains frozen, to compare the quality of the intermediate representations that have been learned using the different pre-training schemes (table 3). 

| Pretraining Dataset  || Pascal mAP (%)  || COCO mAP (%)  || None (Baseline)  || 11.7 (±0.2)  || 5.5 (±0.1)  || OI  || 19.4 (±0.3)  || 9.5 (±0.1)  || IA  || 13.8 (±0.4)  || 6.4 (±0.2)  || PA  || 13.5 (±0.2)  || 6.7 (±0.1)  || IA ∪ OI  || 16.0 (±0.3)  || 7.4 (±0.2)  || IA ∪ PA  || 14.6 (±1.0)  || 6.7 (±0.5)  || PA ∪ OI  || 15.8 (±0.7)  || 7.3 (±0.4)  || OI ∪ IA ∪ PA  || 16.4 (±0.6)  || 7.6 (±0.2)  |


While all pre-training schemes increase the performance, the relative increase for the OI dataset is remarkably higher. This suggests that the style similarity between the IA and PA datasets and our target dataset is less important than we expected. We can not yet conclude whether the superior performance of the OI dataset is due to the similarity in target categories. It could also be caused by other properties of the dataset. Further ablations, e. g., varying the set of OI categories are needed to more precisely assess the impact of category similarity on the detection performance, which we plan to conduct in a follow-up study. Interestingly, the performance of the merged datasets increases even in cases where OI is not part of the dataset merge. Given that we did not apply a sophisticated merging strategy,the performance increase for training with merged datasets is encouraging. Developing strategies to improve the consistency of the merged dataset, e. g., weak labeling of categories not present in the respective merge partners, represents another promising line of future research. We conclude that including an additional stage of object-detection pre-training can lead to a considerable increase in detection performance. While our experiments suggest that style similarities between pre-training and target dataset are less important than matching categories, further experiments are needed to verify this hypothesis. 

This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 101004469 


# notes

[^1]: https://odeuropa.eu
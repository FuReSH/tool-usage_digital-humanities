





# Introduction 


Given the central place of the embodied mind in experientialist approaches to language, co-verbal gestures have become a valuable data source in cognitive, functional and anthropological linguistics (e.g. ). While there exist various views on embodiment, the core idea is that human higher cognitive abilities are shaped by the morphology of our bodies and the way we interact with the material, spatial and social environment (e.g. ; ). Drawing on these premises, some gesture scholars stress that gestures are conditioned by the forms and affordances of their material habitat as well as the speakers’ interactive and collaborative practices (e.g. ; ). Pioneering work done by Kendon (e.g. , ), McNeill (e.g. , , ) and Müller has shown that manual gestures are an integral part of utterance formation and communicative interaction. The state-of-the-art of research in the growing interdisciplinary field of gesture studies has recently been presented in the International Handbook on Multimodality in Human Interaction ( , Vol. 1 2013, Vol. 2 2014). One quintessence to be drawn from this large body of work is that language, whether spoken or signed, is embodied, dynamic, multimodal and intersubjective (see also ; ; ; ; ). 

Indeed, human communication typically involves multiple modalities such as vocalizations, spoken or signed discourse, manual gestures, eye gaze, body posture and facial expressions. In face-to-face communication, manual gestures play an important role by conveying meaningful information and guiding the interlocutors’ attention to objects and persons talked about. Gestures here are understood as spontaneously emerging, dynamic configurations and movements of the speakers’ hands and arms that contribute to the communicative content and partake in the interactive organization of a spoken dialogue situation (e.g. ; ; ; ; ). The contribution of gestures to multimodal interaction may consist in, e.g., deictic reference to locations, ideas, persons or things (both abstract and concrete); they may also fulfill metalinguistic functions, e.g., in referring to citations of other speakers or outlining the structure of their argumentation. Gestures may also provide schematic, iconic portrayals of actions, things or spatial constellations ; . As we use the term here, gestures are not to be confused with emblems (e.g. the victory sign), which have a culturally defined form-meaning correlation in the same sense that words may have a fixed meaning . So gestures are always to be investigated in view of the co-occurring speech with which they jointly create (new) semiotic material and support speakers in organizing their thoughts or drawing connections to their social and physical environment ; . 

Drawing on this large body of gesture research across various fields of the humanities and social sciences, the interdisciplinary approach presented here aims at identifying and visualizing patterns of gestural behavior with the help of custom-tailored computational tools and methods. Although co-speech gestures tend to be regarded as highly idiosyncratic in respect to their spontaneous individual articulation by speakers in spoken dialogue situations, it is safe to assume that there are recurring forms of dynamic hand configurations and movement patterns which are performed by speakers sharing the same cultural background. From this assumption follows the hypothesis that, on the one hand, a general degree of similarity between gestural forms may be presumed – trivially – due to the shared morphology of the human body (e.g. ). On the other hand, the specific cultural context plays an important role in both language acquisition and the adoption of culture-specific behavioral patterns (see, e.g. on Habitus and Hexis). Moreover, gesture has also been ascribed a constitutive role regarding the human capacity for language both ontogenetically and phylogenetically (e.g. ; ). Previous empirical gesture research indeed shows that co-verbal gestures exhibit recurrent form features and movement patterns, as well as recurring form-meaning pairings: see, for instance, Bressem on form features; Fricke on "kinaesthemes "; Kendon on "locution clusters ", McNeill , on cross-linguistic path and manner imagery portraying motion events; McNeill on "catchments "; Ladewig and Müller on "recurring gestures ", and Cienki and Mittelberg on image-schematic patterns. From this perspective, a question central to gesture research concerns the factors that may motivate and lend a certain systematicity to forms and functions of human communicative behaviors not constituting an independent sign system in the Saussurian sense as spoken and signed languages do. Manual gesture is a semiotically versatile medium, for it may, depending on a given context, assume more or less language-like functions: from accompanying spoken discourse in the form of pointing, accentuating beats, schematic iconicity or managing social interaction to carrying the full load of language (e.g. ). 

In this paper, we will focus on certain kinds of co-verbal gestures, i.e. specific image-schematic gestalts, e.g. spirals, circles, and straight paths , . Figure 1 shows visualizations of several different gestural traces belonging to these movement types. In what follows, we will present a novel method designed to identify, search and cluster in an automatized fashion gestural movement patterns throughout our data set, and potentially also any other motion capture data set, e.g. to recognize and group those traces that are similar regarding their formal features and the way in which they unfold in gesture space. 


# Research Objective 


Whereas the gesture research discussed above mostly relies on observational methods and qualitative video analyses, our aim is to add to the catalogue of methods for empirical linguistics and gesture studies by outlining a computational, quantitative and comparative 3D-model driven approach in gesture research. While there is a trend to combine qualitative with quantitative as well as experimental methods in multimodal communication research ; ; , standardized and widely applicable tools and methods still need to be developed. In order to derive statistically significant patterns from aligned linguistic and behavioral data, some recent research initiatives have started to compile and work with larger-scale corpora, e.g. drawing on technological advances in data management and automated analyses (e.g. the Little Red Hen Project, ). While using audio and video technology to record co-speech gestures remains the dominant way to construct multimodal corpora, some research teams have begun to employ 3D motion capture technology to overcome the limits of 2D video, e.g. Lu and Huenerfauth for signed language, and Beskow et al. and Pfeiffer et al. for spoken discourse (see also for an overview). Our contribution is to obtain some kind of numerical instrument for graduating gestural similarity for measuring gesture similarity in sets of recorded behavioral data. While this instrument, in its current state, in no way addresses the problem of the meaning of certain gestural forms, it is the first step towards a model of measuring similarity between recurring dynamic gesture form patterns. Our goal is to first establish a robust, flexible and automated methodology which allows us to determine 

- whether there are shared or common reoccurring gestural movement patterns in a given set of 3D recorded, behavioral communication data, 
- exactly which forms there are, and 
- the extent to which they occur, 

and then to apply this methodology to the recorded 3D numerical MoCap data of a group of participants. 

Both the alignment of gestures with the co-occurring speech, and the semantic comparison of the established (formally) sufficiently similar gesture-speech constructions, still have to be done manually by human gesture researchers, through semiotic analyses of the multimodal, speech and behavioral data corpora. The primary aim of developing an automated indicator of gesture similarity is to identify recurrent movement patterns of interest from the recorded 3D corpus data computationally, and thus to enable human gesture researchers to handle these data sets in a more efficient manner. In order to make gesture similarity automatically accessible, we propose a distance-based similarity model for gestures arising in three-dimensional motion capture data streams. In comparison to two-dimensional video capture technology, working with numerical three-dimensional motion capture technology has the advantage of measuring and visualizing the temporal and spatial dynamics of otherwise invisible movement traces with the highest possible accuracy. We aim at maintaining this accuracy by aggregating movement traces, also called trajectories, into a gesture signature . This gesture signature has the ability of weighting trajectories according to their relevance. Based on this lossless feature representation, we propose to measure similarity by means of distance-based approaches , . We particularly investigate the Earth Mover's Distance and the Signature Quadratic Form Distance for the comparison of two gesture signatures on real conversational motion capture data. 


# Properties of the 3D Data Model 


From a philosophy of science point of view, before being able to apply computational algorithms to naturalistic real-world gestures, there must be a translation from the real-world dialogue situations, involving people speaking and gesturing, from which data are captured, to a computable set of data. For this purpose, a marker-based Vicon Motion Capture system was used in this study. Participants wear a series of markers attached to predetermined body parts of interest (fingers, wrists, elbows, neck, head, etc.). The Vicon system automatically generates a chart of numerical 4-tuples of Euclidean space-time coordinates for each marker attached to these points on the participants’ bodies. The movement of the markers is tracked by 14 Vicon infrared cameras, and the physical trajectories of the markers are represented in a chart of space-time coordinates. These space-time charts form the data sets that are investigated algorithmically, relieving the gesture analyst of the difficult, and subjective, task of manually examining highly ephemeral real-world dialogue situations. But what are the crucial features that such a numerical representation must have in order to enable researchers to not only investigate a model but also to finally derive statements and theories about a modeled real-world situation? We address the following research questions: Which logical features of the model are essential if one wants to investigate the real world by investigating a model? And secondly, what are the epistemic benefits of investigating models instead of real-world situations? 

The most important feature is that the model represents its representandum. The representandum in question is a set of relevant features and relations of a given part of reality, namely the change in space-time location of certain body parts caused by the participants’ kinetic movement. The representing model, on the other hand, is the virtual, computational 3D recording of the real-world kinetic movement, mapped onto Euclidean space. Representation itself is the relation which holds between the model and its representandum. Representation, as we understand the term here, is a non-reflexive and non-symmetric relation, which simply means that an a does not necessarily represent a , and that if a represents b , then b does not necessarily represent a . We further assume that representation depends on transitive relations, such as identity between some complex relational features of the entities to be represented in a model and the entities that represent/model them. In short, this means that if there are entities x, y, z, there must be at least one mutual complex relational feature Fr(x,y,z), which these entities have in common. The transitive relation R , then, is the "identity "relation, in that two entities have an identical relation to a third entity – a frame of reference, or a tertium comparationis . 

Definition: Transitivity 

For a binary relation R over a set A consisting of x, y, z, the following statement holds true: 

∀x,y,z ∈ A: xRy & yRz → xRz 

The transitivity of R is so important here because, in terms of modeling, it is the crucial feature that R must have. The different relata involved are: movements of body parts (x) , movements of markers (y) , and computational trajectories (z) . The relation R which holds between these relata is the identity of their curve with respect to space – either to a given virtual Euclidian space, or the physical space (which functions as a reference frame). The identity of the movement curve of body parts and the movement curve of markers simply stems from their physical attachment/ conjunction in physical space. The identity of marker movements and the computational trajectories is a result of metering the markers’ light reflections, by means of the 14 Vicon infrared cameras, and numerically mapping the outcome onto Euclidean space coordinates. But if one differentiates between physical and Euclidean space, there is hardly any identity one could honestly speak of in this case. In what sense could physical and virtual movement curves ever be identical? Only in the sense that we identify physical and Euclidean space by conventions of scientific modeling and metering practices; the term curve is itself an indication of how familiar that convention is. Since Euclidean space is a conventionalized and well-accepted geometrical model of physical space – we are well accustomed to talking about physical movement in terms of, distances, trajectories, vectors, miles, kilometres and change in space/time coordinates etc. – one can say that Euclidean space is our familiar standard model for describing our perception of movement in physical space, both in everyday conversation and scientific discussion. Thus, it is justified to speak of the identity of the spatial curves of trajectories in Euclidean space and those of moving physical objects such as Motion Capture markers, only if we accept this convention. Regarding representation, what does this imply? If representation depends on the transitivity of relation R holding between the entities x, y, z, and the identity of that relation depends on conventions of metering and modeling, representation additionally depends on following conventions. Given a 4-tuple of coordinates of a MoCap marker, the movement of this marker is modeled by a vector which points from tuple-1 to tuple-2 to tuple-n. The crucial feature of this kind of modeling is that the movement of a single marker a is represented as a dynamic space-time trajectory which aggregates the consecutively changing coordinates of a over a given time frame. 

Regarding the above-mentioned definition of transitivity, let our variables take the following values: 

x = movement of body part from position a to b;  y = movement of marker M from position a to b;  z = trajectory of marker M 

Given these values, we outline the transitivity relation as follows: 

∀x,y,z ∈ A: xRy & yRz → xRz: x [movement of body part from position a to b] R y [movement of marker M from position a to b] & y [movement of marker M from position a to b] R z [trajectory of marker M] → x [movement of body part from position a to b] R z [trajectory of marker M]. 

This means that if x, y, z obtain a transitive relation R in the above sense, then x, y, and z are to be regarded as homomorphous abstract concepts that all denote the same event of spatiotemporal movement, and R is an equivalence relation. So the extensional equivalence of these concepts is a necessary and sufficient condition that allows us to investigate reality by investigating the model: If a language and its translation are equivalent, it should be equally valid to investigate one or the other. However, since the translation of the concept "movement of marker "into "aggregated marker coordinates "fails to be an intensionally adequate translation, i.e. the concepts do not mean the same (event vs. aggregated states of affairs), it could at first seem odd to describe real movement in terms of trajectories. But, since the concepts are at least phenomenologically and extensionally equivalent, this basically remains a question of the interpreter’s ontology (see ) and how the final research result is to be formulated. If we decide to treat "event "and "aggregated states of affairs "as being synonymous, the problem completely disappears. Otherwise, we have to re-translate the problematic concept into one which suits our needs. In terms of epistemic benefits, one major advantage of the proposed distance-based gesture-similarity model (see the following section), i.e. the combination of gesture signatures with signature-based distance functions, is its applicability to any type of gestural pattern and to data sets of any size. In fact, distance-based similarity models can be utilized in order to model similarity between gestural patterns whose movement types are well known and between gestural patterns whose inherent structures are completely unknown. In this way, they provide an unsupervised way of modeling gesture similarity. This flexibility is attributable to the fact that the proposed approaches are model independent, i.e. no complex gesture model has to be learned in a comprehensive training phase prior to indexing and query processing. Another advantage of the proposed distance-based gesture-similarity model is the possibility of efficient query processing. Although calculating the distance between two gesture signatures is a computationally expensive task, which results in at least a quadratic computation time complexity with respect to the number of relevant trajectories, many approaches such as the independent minimization lower bound of the Earth Mover's Distance on feature signatures and metric indexing , as well as the Ptolemaic indexing of the Signature Quadratic Form Distance, are available for efficient query processing and, thus, for assessing gesture similarity in a larger quantitative way. 


# Modeling Gesture Similarity 


In this section, we present a distance-based similarity model for the comparison of gestures within three-dimensional motion capture data streams. To this end, we first introduce gesture signatures as a formal model of gestures arising in motion capture data streams. Since gesture signatures comprise multiple three-dimensional trajectories, we continue with outlining distance functions for trajectories before we investigate distance functions applicable to gesture signatures. 


## Gesture Signatures 


Motion capture data streams can be thought of as sequences of points in a three-dimensional Euclidean space. In the scope of this work, these points arise from several reflective markers which are attached to the body and in particular to the hands of a participant. The motion of the markers is triangulated via multiple cameras and finally recorded every 10 milliseconds. In this way, each marker defines a finite trajectory of points in a three-dimensional space. The formal definition of a trajectory is given below. 

Definition: Trajectory 

Given a three-dimensional feature space R 3 , a trajectory t :{1,…,n}→ R 3 is defined for all 1≤i≤n as: 

t (i)=(x i ,y i ,z i ) 

A trajectory describes the motion of a single marker in a three-dimensional space. It is worth noting that the time information is abstracted to integral numbers in order to model trajectories arising from different time intervals. Since a gesture typically arises from multiple markers within a certain period of time, we aggregate several trajectories including their individual relevance by means of a gesture signature. For this purpose, we denote the set of all finite trajectories as trajectory space T=∪ k∈N {t| t:{1,…,k}→ R 3 } , which is time-invariant, and define a gesture signature as a function from the trajectory space T into the real numbers R. The formal definition of a gesture signature is given below. 

Definition: Gesture Signature 

Let T be a trajectory space. A gesture signature S ∈R T is defined as: 

S:T→ R subject to |S -1 (R{0})|<∞ 

A gesture signature formalizes a gesture by assigning a finite number of trajectories non-zero weights reflecting their importance. Negative weights are immaterial in practice but ensure the gesture space S={ S ∈R T ∧|S -1 (R{0})|<∞} forms a vector space. While a weight of zero indicates insignificance of a trajectory, a positive weight is utilized to indicate contribution to the corresponding gesture. In this way, a gesture signature allows us to focus on the trajectories arising from those markers which actually form a gesture. For example, if a gesture is expressed by the participant's hands, only the corresponding hand markers and thus trajectories have to be weighted positively. 

A gesture signature defines a generic mathematical model but omits a concrete functional implementation. In fact, given a subset of relevant trajectories 𝒯 + ⊂T, the most naive way of defining a gesture signature S consists in assigning relevant trajectories a weight of one and irrelevant trajectories a weight of zero, i.e. by defining S for all t ∈T as follows: $$S(t) = \left\{ \begin{matrix} 1, \ if\ t \in \mathcal{T}^{+} \\ 0, \ \text{otherwise} \\ \end{matrix} \right.$$ 

The isotropic behavior of this approach, however, completely ignores the inherent characteristics of the relevant trajectories. We therefore weight each relevant trajectory according to its inherent properties of motion distance and motion variance . These properties are defined below. 

Definition: Motion Distance and Motion Variance 

Let T be a trajectory space and t :{1,…,n}→ R 3 be a trajectory. The motion distance m δ :T→R of trajectory t is defined as: $$m_{\delta}(t) = \sum_{i = 1}^{n - 1}\left\| t(i) - t(i + 1) \right\|_{2}$$ 

The motion variance m σ 2 :T→R of trajectory t is defined with mean $$m_{\sigma^{2}}(t) = \frac{1}{n} \cdot \sum_{i = 1}^{n}\left\| t(i) - \mu(t) \right\|_{2}^{2}$$ 

as: $$S_{m_{\delta}}(t) = \left\{ \begin{matrix} m_{\delta}(t), \ if\ t \in \mathcal{T}^{+} \\ 0, \ \text{otherwise} \\ \end{matrix} \right.$$ 

The intuition behind motion distance and motion variance is to take into account the overall movement and vividness of a trajectory. The higher these qualities, the more information the trajectory may contain and vice versa. Their utilization with respect to a set of relevant trajectories finally leads to the definitions of a motion distance gesture signature and a motion variance gesture signature , as shown below. 

Definition: Motion Distance Gesture Signature and Motion Variance Gesture Signature 

Let T be a trajectory space and 𝒯 + ⊂T be a subset of relevant trajectories. A motion distance gesture signature S m δ ∈R T is defined for all t ∈T as: $$S_{m_{\sigma^{2}}}(t) = \left\{ \begin{matrix} m_{\sigma^{2}}(t), \ if\ t \in \mathcal{T}^{+} \\ 0, \ \text{otherwise} \\ \end{matrix} \right.$$ 

A motion variance gesture signature S m σ 2 ∈R T is defined for all t ∈T as: $$DTW_{\delta}\left( t_{n},t_{m} \right) = \delta\left( t_{n}(n),t_{m}(m) \right) + \min_{}\left\{ \begin{matrix} DTW_{\delta}\left( t_{n - 1},t_{m - 1} \right) \\ DTW_{\delta}\left( t_{n},t_{m - 1} \right) \\ DTW_{\delta}\left( t_{n - 1},t_{m} \right) \\ \end{matrix} \right.$$ 

Motion distance and motion variance gesture signatures are able to reflect the characteristics of the expressed gestures with respect to the corresponding relevant trajectories by adapting the number and weighting of relevant trajectories. As a consequence, the computation of a (dis)similarity value between gesture signatures is frequently based on the (dis)similarity values among the involved trajectories in the trajectory space. We thus outline applicable trajectory distance functions in the following section. 


## Trajectory Distance Functions 


Due to the nature of trajectories whose inherent properties are rarely expressible in a single figure, trajectories are frequently compared by aligning their coincident similar points with each other. A prominent example is the Dynamic Time Warping Distance , which was first introduced in the field of speech recognition by Itakura and Sakoe and Chiba and later brought to the domain of pattern detection in databases by Berndt and Clifford . The idea of this distance is to locally replicate points of the trajectories in order to fit the trajectories to each other. The point-wise distances finally yield the Dynamic Time Warping Distance, whose formal definition is given below. 

Definition: Dynamic Time Warping Distance 

Let t n :{1,…,n}→ R 3 and t m :{1,…,m}→ R 3 be two trajectories from T and δ :R 3 ×R 3 →R be a distance function. The Dynamic Time Warping Distance DTW δ :T×T→R between t n and t m is recursively defined as: 

- \(DTW_{\delta}\left( t_{0},t_{0} \right) = 0\) 
- \(DTW_{\delta}\left( t_{i},t_{0} \right) = \infty\ \ \forall 1 \leq i \leq n\) 
- \(DTW_{\delta}\left( t_{0},t_{j} \right) = \infty\ \ \forall 1 \leq j \leq m\) 

with $$EMD_{\delta}\left( S_{1},S_{2} \right) = \min_{F}\left\{ \frac{\Sigma_{t \in \mathbb{T}}\Sigma_{t'\mathbb{\in T}}\delta(t,t') \cdot f(t,t')}{\min_{}\left\{ \Sigma_{t\mathbb{\in T}}S_{1}(t),\Sigma_{t'\mathbb{\in T}}S_{2}(t') \right\}} \right\}$$ 

As can be seen in the definition above, the Dynamic Time Warping Distance is defined recursively by minimizing the distances δ between replicated elements of the trajectories. In this way, the distance δ assesses the spatial proximity of two points while the Dynamic Time Warping Distance preserves their temporal order within the trajectories. By utilizing Dynamic Programming, the computation time complexity of the Dynamic Time Warping Distance lies in 𝒪(n·m). 

Although there exist further approaches for the comparison of trajectories, such as Edit Distance on Real Sequences , Minimal Variance Matching , and Mutual Nearest Point Distance , we have decided to utilize the Dynamic Time Warping Distance for the following reasons: (i) The distance value is based on all points of the trajectories with respect to their temporal order and is not attributed to partial characteristics of the trajectories, (ii) it provides the ability of exact indexing by lower bounding , and (iii) it indicates superior quality in terms of accuracy within preliminary investigations. 

Given a ground distance in the trajectory space T, we will show in the following section how to lift this ground distance to the gesture space S⊂R T in order to compare gesture signatures with each other. 


## Gesture Signature Distance Functions 


Gesture signatures can differ in size and length, i.e., in the number of relevant trajectories and in the lengths of those trajectories. In order to quantify the distance between differently structured gesture signatures, we apply signature-based distance functions , . In this paper, we focus on those signature-based distance functions that consider the entire structure of two gesture signatures in order not to favor partial similarity between short and long gesture signatures. For this reason, we investigate the transformation-based Earth Mover's Distance and the correlation-based Signature Quadratic Form Distance in the remainder of this section. 

The Earth Mover's Distance, whose name was inspired by Stolfi and his vivid description of the transportation problem, which he likened to finding the minimal cost to move a total amount of earth from earth hills into holes , has been originated in the computer vision domain. It defines the distance between two gesture signatures by measuring the cost of transforming one gesture signature into another one. The formal definition of the Earth Mover's Distance is given below. 

Definition: Earth Mover’s Distance 

Let S 1 , S 2 ∈S be two gesture signatures and δ :T×T→R be a trajectory distance function. The Earth Mover’s Distance EMD δ :S×S→R between S 1 and S 2 is defined as a minimum cost flow of all possible flows F ={f| f: T×T→R} as: 

- \(\forall t,t'\mathbb{\in T};f(t,t') \geq 0\) 
- \(\forall t \in \mathbb{T};\Sigma_{t'\mathbb{\in T}}f(t,t') \leq S_{1}(t)\) 
- \(\forall t'\mathbb{\in T};\Sigma_{t\mathbb{\in T}}f(t,t') \leq S_{2}(t')\) 
- \(\Sigma_{t \in \mathbb{T}}\Sigma_{t'\mathbb{\in T}}\ f(t,t') = \min_{}\left\{ \Sigma_{t \in \mathbb{T}}S_{1}(t),\Sigma_{t'\mathbb{\in T}}S_{2}(t') \right\}\) 

subject to the constraints: $$\left\langle S_{1},S_{2} \right\rangle_{s} = \sum_{t \in \mathbb{T}}^{}{\sum_{t'\mathbb{\in T}}^{}{S_{1}(t) \cdot S_{2}(t') \cdot s(t,t')}}$$ 

As can be seen in the definition above, the Earth Mover's Distance between two gesture signatures is defined as a linear optimization problem subject to non-negative flows which do not exceed the corresponding limitations given by the weights of the trajectories of both gesture signatures. The computation of the Earth Mover's Distance can be restricted to the relevant trajectories of both gesture signatures and follows a specific variant of the simplex algorithm . 

The idea of the Signature Quadratic Form Distance consists in adapting the generic concept of correlation to gesture signatures. In general, correlation is the most basic measure of bivariate relationship between two variables and can be interpreted as the amount of variance these variables share . In order to apply the concept of correlation to gesture signatures, all trajectories and corresponding weights are related with each other based on a trajectory similarity function s :T×T→R. The resulting similarity correlation between two gesture signatures S 1 , S 2 ∈S is then defined as: $$SQFD_{s}\left( S_{1},S_{2} \right) = \sqrt{\left\langle S_{1},S_{1} \right\rangle_{s} - 2 \cdot \left\langle S_{1},S_{2} \right\rangle_{s} + \left\langle S_{2},S_{2} \right\rangle_{s}}$$ 

The similarity correlation between two gesture signatures finally leads to the definition of the Signature Quadratic Form Distance, as shown below. 

Definition: Signature Quadratic Form Distance 

Let S 1 , S 2 ∈S be two gesture signatures and s :T×T→R be a trajectory similarity function. The Signature Quadratic Form Distance SQFD s :S×S→R between S 1 and S 2 is defined as: $$SQFD_{s}\left( S_{1},S_{2} \right) = \sqrt{\left\langle S_{1},S_{1} \right\rangle_{s} - 2 \cdot \left\langle S_{1},S_{2} \right\rangle_{s} + \left\langle S_{2},S_{2} \right\rangle_{s}}$$ 

The Signature Quadratic Form Distance is defined by adding the intra-similarity correlations < S 1 , S 1 > s and < S 2 , S 2 > s of the gesture signatures S 1 and S 2 and subtracting their inter-similarity correlation < S 1 , S 2 > s . The smaller the differences among the intra-similarity and inter-similarity correlations the lower the resulting Signature Quadratic Form Distance, and vice versa. The computation of the Signature Quadratic Form Distance can be restricted to the relevant trajectories of both gesture signatures and has a quadratic computation time complexity with respect to the number of relevant trajectories. 

More details regarding the Earth Mover's Distance and the Signature Quadratic Form Distance as well as possible similarity functions can be found for instance in the PhD thesis of Beecks . Among other approaches, such as the matching-based Signature Matching Distance , the aforementioned signature-based distance functions have been shown to balance the trade-off between retrieval accuracy and query processing efficiency. Before we investigate their performance in the context of gesture signatures, we devote the next section to a discussion about the properties of the proposed distance-based gesture similarity model. 


# Experimental Evaluation 


Evaluating the performance of distance-based similarity models is a highly empirical discipline. It is nearly unforeseeable which approach will provide the best retrieval performance in terms of accuracy. To this end, we qualitatively evaluated the proposed distance-based approaches to gesture similarity by using a natural media corpus of motion capture data collected for this project. This dataset comprises three-dimensional motion capture data streams arising from eight participants during a guided conversation. The participants were equipped with a multitude of reflective markers which were attached to the body and in particular to the hands. The motion of the markers was tracked optically via cameras at a frequency of 100 Hz. In the scope of this work, we used the right wrist marker and two markers attached to the right thumb and right index finger each. The gestures arising within the conversation were classified by domain experts according to the following types of movement: spiral, circle, and straight. Example gestures of these movement types are sketched in Figure 1. A total of 20 gesture signatures containing five trajectories each was obtained from the motion capture data streams. The trajectories of the gesture signatures have been normalized to the interval [0,1] 3 ∈R 3 in order to maintain translation invariance. 

The resulting distance matrices between all gesture signatures with respect to the Earth Mover's Distance and the Signature Quadratic Form Distance are shown in Figure 2 and Figure 3, respectively. As described in the previous Section, we utilized the Dynamic Time Warping Distance based on Euclidean Distance as trajectory distance for the Earth Mover's Distance and converted this trajectory distance by means of the power kernel with parameter α =1 into a trajectory similarity function for the Signature Quadratic Form Distance. Since weighting of relevant trajectories by motion distance and motion variance, approximately shows a similar behavior, we include the results regarding motion variance gesture signatures only. We depict small and large distance values by bluish and reddish colors in order to visually indicate the performance of our proposal: gesture signatures from the same movement type should result in bluish colors while gesture signatures from different movement types should result in reddish colors. 

As can be seen in Figure 2 and Figure 3, both Earth Mover's Distance and Signature Quadratic Form Distance show the same tendency in terms of gestural dissimilarity. Although distance values computed through the aforementioned distance functions have different orders of magnitude, both gesture signature distance functions are generally able to distinguish gesture signatures from different movement types. On average, gesture signatures belonging to the same movement type are less dissimilar to each other than gesture signatures from different movement types. We further observed that the distinction between gesture signatures from the movement types spiral and straight are most challenging. This is caused by a similar sequence of movement of these two gestural types. While gesture signatures belonging to the movement type straight follow a certain direction, e.g., movement on the horizontal axis, gesture signatures from the movement type spiral additionally oscillate with respect to a certain direction. Since this oscillation can be dominated by the movement direction, the underlying trajectory distance functions are often unable to distinguish oscillating from non-oscillating trajectories and thus gesture signature of movement type spiral from those of movement type straight. 

Apart from the quality of accuracy, efficiency is another important aspect when evaluating the performance of gesture similarity models. For this purpose, we measured the computation times needed to perform single distance computations on a single-core 3.4 GHz machine. We implemented the proposed distance-based approaches in Java 1.7. The Earth Mover's Distance, which needs on average 148.6 milliseconds for a single distance computation, is approximately three times faster than the Signature Quadratic Form Distance, which needs on average 479.8 milliseconds for a single distance computation. In spite of the theoretically exponential and empirically super-cubic computation time complexity of the Earth Mover's Distance , this distance is able to outperform the Signature Quadratic Form Distance. The reason for this is the high number of computationally expensive trajectory distance computations. While the computation of the Earth Mover's Distance is carried out on the trajectory distances between the two gesture signatures, the computation of the Signature Quadratic Form Distance additionally takes into account the trajectory distances within both gesture signatures. Therefore the number of trajectory distance computations is significantly higher for the Signature Quadratic Form Distance than for the Earth Mover's Distance. 

To sum up, the experimental evaluation reveals that the proposed distance-based approaches are able to model gesture similarity in a flexible and model-independent way. Without the need for a preceding training phase, the Earth Mover's Distance and the Signature Quadratic Form Distance are able to provide similarity models for searching similar gestures which are formalized through gesture signatures. 


# Conclusions and Future Work 


In this paper, we have investigated distance-based approaches to measure similarity between gestures arising in three-dimensional motion capture data streams. To this end, we have explicated gesture signatures as a way of aggregating the inherent characteristics of spontaneously produced co-speech gestures and signature-based distance functions such as the Earth Mover's Distance and the Signature Quadratic Form Distance in order to quantify dissimilarity between gesture signatures. The experiments conducted on real data are evidence of the appropriateness in terms of accuracy and efficiency of the proposal. 

In future work, we intend to extend our research on gesture similarity towards indexing and efficient query processing. While the focus of the present paper lies on dissimilarity between pairs of gestures, we further plan to quantitatively analyze motion capture data streams in a query-driven way in order to support the domain experts' qualitative analyses of gestural patterns within multi-media contexts. The overall goal of this research is to contribute to the advancement of automated methods of pattern recognition in gesture research by enhancing qualitative analyses of complex multimodal data in the humanities and social sciences. While this paper focuses on formal features of the gestural movements, further steps will entail examining the semantic and pragmatic dimensions of these patterns in light of the cultural contexts and embodied semiotic practices they emerge from. 


# Acknowledgment 


This work is partially funded by the Excellence Initiative of the German federal and state governments and DFG grant SE 1039/7-1. This work extends . 


# notes

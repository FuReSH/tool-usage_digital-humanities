
The digital revolution has changed the approach to research, especially for humanities. In the last decades the process of image scanning, transcription and creation of digital archives of text has materialized, but many scholars aren't conscious of the results it can achieve for their studies. For that reason, Analysis of Ancient and Medieval Texts and Manuscripts: Digital Approaches aims to show the possibilities given by computer-assisted methods in the analysis of ancient and medieval codices. In particular, there is a focus on the combination and comparison of data, which can lead to better results than in the past. Even if the publication dates to 2014 and in these last few years the research has improved, the volume is an important instrument to explore the world of digital humanities and its potential contributions to research. The book is divided into five sections – each of them dedicated to a certain aspect of manuscripts’ analysis. As explained in the acknowledgements, the articles contained in this compiled volume were presented in a workshop organized on April 2-3 2012 in Leuven and Brussels. 

The four essays of the first section, Stemmatology, recognize the importance of computer-assisted methods, but suggest combining them with the classical approach elaborated by Karl Lachmann and Paul Maas. The first article, by Tuomas Heikkilä, shows the results of using six algorithms to know the textual tradition of the first ten chapters of the Vita et miracula s. Symeonis: RHM, Semstem, R&B, MP, NJ and NeighborNet. All the algorithms revealed three large groups which correspond to the preliminary grouping based on Lachmannian-Maasian method, but there are some difficulties with contamination of the text. The same problem occurs in the following article by Philipp Roelli, who analyzed Petrus Alfonsi’s Dialogus and recognized that the algorithm had not been able to distinguish many Leitfehler (or errores significativi ). 

The third and the fourth essays illustrate similar studies, with the purpose "to extend philology through the aid of the computer ": Jean-Baptiste Camps and Florian Cafiero applied the algorithm to Li Bestiaires d'Amours of Richart de Fornival to create a stemma codicum and compare it with the true stemma; Alberto Cantera, meanwhile, analyzed the manuscripts of the Avestan corpus, with all the difficulties related to a text composed orally and transmitted in that way for a long time. We can say, based on their experiments, that the computer-assisted method has the advantage of saving time during the analysis, but it still needs improvement: therefore the suggestion is to combine it with the classical approach, as they compensate each other. In fact, they use the same material and the same principles, but in a different way and with different achievements. So we stand to benefit from the possibilities that the two approaches offer, promising the same result in the study of a document in the future. 


The true stemma of Li Bestiaires d'Amours of Richart de Fornival and the one created by Camps and Cafiero with an algorithm 


The second section, Statistics and Stylistics, comprises three articles referring to the application of statistical methods in analysis of a text. The first one, by Armin Hoenen, illustrates the experiment of producing an artificial tradition by using an algorithm (ATG), in order to develop a potent stemmatology. The algorithm, as if it was a copyist, has analyzed an input manuscript and introduced several letter changes to the copy it has produced, with the purpose being to get an output corpus similar to real corpora. This study is only the first step, but it has revealed that copyists tended to make mistakes for several reasons, not only for the resemblance of the letters: in fact there are other influencing factors like their proficiency, mother tongue or concentration of the scribe. Just at the beginning, first results have referred to single letters, and next Hoenen wants to investigate the distribution of words and variants. 

An interesting experiment is made by Karina van Dalen-Oskam and described in the second essay. Its aim was to consider Jacob van Maerlant’s Scolastica , trying to distinguish the text edited by modern editors from the one transmitted by medieval codices. The results have shown the similarities between the two versions, which is not surprising, but can be useful to recognize the attitudes towards the texts. Therefore van Dalen-Oskam, referencing the article The Scribe as Editor by Elspeth Kennedy , notes that the experiment gives a not-very-convenient conclusion: the scribe is like an editor who made choices for the transmission, just as nineteenth-century and contemporary editors do. For this reason, in order to prepare new editions, we should be aware of the importance of scribes’ work, and not only of the text itself: recalling the title of a volume by Leighton D. Reynolds and Nigel G. Wilson, texts and transmission are two essentials elements for philological studies and the one necessarily implies the other . 

The last article of this section is by Francesco Stella, with the collaboration of Luca Verticchio and Stefania Pennasilico. Considering the lexicon of epistolary corpora in Latin from the republican to the humanistic period (Cicero, Pliny the Younger, Augustine, Alcuin of York, Abelard and Petrarch), the study wanted to understand the historical development of the Latin language using the program Analisi lessicale. It permitted identifying the most frequent terms ( et , in , non , est , ut ), the empty terms ( enim is one of the most used) or the variation of forms in the different authors analyzed, in order to make statistical conclusions. Starting from this experiment the hope is to deepen the relationships between diction and social-cultural communication, with the purpose to formulate a general hypothesis related to the literary language. 


Stella’s ten most common terms in Cicero, Pliny the Younger, Augustine and Alcuin of York 


The third section, Intertextuality, focuses on the possibilities offered by several digital archives – important sources to analyze a text and its tradition. One of them is the project Musisque Deoque, dedicated to Latin poetry and described in the first essay by Linda Spinazzè. The digital archive is useful for specific researches towards the document, like filtering specific meters out or finding words in a particular position of the verse, but it also allows inserting a digital edition into the corpus. Therefore Spinazzè has shown the way to edit Maximianus’ elegiac work for Musisque Deoque, noting the possibility to read the critical apparatus, to interrogate variants and to get information related to a specific codex. Its instrument is the prototype for the creation of a cyberspace for classical literature, which represents the future of humanities with a collaboration between philology and technology. 

The three following essays explore other digital archives, whose aims are similar to Musisque Deoque: Samuel Rubenson describes the database of the Apophthegmata Patrum, which is able to analyze at various levels the corpus of monastic fathers, with the possibility for a scholar to share his work on a text and to give an updated version of the text itself; Charlotte Tupman and Anna Jordanous talk about the Sharing Ancient Wisdoms (SAWS) project and several methods to link and share the relationship between one wisdom text and another; finally Maxim Romanov illustrates a way to study the Arabical sources by developing text-mining techniques, in order to extract information from a certain document thanks to metadata. 


Musisque Deoque digital archive 


The fourth section, Script analysis, examines the importance of paleography on the study of ancient and medieval manuscripts. Just as philology, the discipline has advanced over the last few years of the digital revolution. In the first essay, Ainoa Castro Correa gives an overview of the paleographical problems to analyze a codex, referring in particular to several tools applied to the Visigothic script. After research in the archives, Castro Correa has created a database of the documentation from the early medieval history of Galicia in order to extract different information from each text: the signature, the type of writing and other notes that can be useful in the study of a codex. Castro Correa has also used Adobe Photoshop to develop analysis of the Visigothic script, because certain elements can be understood only with the use of an image. For this reason the author built a catalogue of graphic forms which included several aspects like the form of the letter or the abbreviations, and then compared all the images. That experiment has allowed the creation of a graphic profile by using metadata, which in the future can be considered the first step to study an ancient or a medieval document. There are obviously some difficulties to overcome – for example the script variations of a certain scribe – but we can be positive that in few years there could be a collaboration between the experienced paleographic eye and digital paleography. 

The second essay of this section, by Eugenio R. Luján and Eduardo Orduña, explores the Hesperia databank of Palaeohispanic languages, which collect the inscriptions from the Iberian Peninsula, excepts for the ones in Latin, Greek and Phoenician. Therefore it is explained how to navigate in the database and what the possibilities are offered: it is possible to do researches in a text and see its variants, and also to look for lexical and bibliographical references by using specific entries. 


The Hesperia databank of Palaeohispanic languages 


The last section of the book, Codicology, refers to the manuscript itself, seen as the medium by which we know ancient and medieval literature. Its two articles show the collaboration between the discipline and the most recent technologies, in order to aid greatly the study of writing. The first one, by Ira Rabin, discusses the possibility to employ ink identification to create a database which could give more information during the analysis of a codex. After explaining the different types of ink (soot, plant, iron-gall and inks of a mixed composition), there is an overview of the analytic procedures which can be used to identify a certain ink: the Vibrational spectroscopy, the analysis by X-ray fluorescence technique, and the multi-color microscope. 

The second article, by Patrick Andrist, starts with a question asked by the author to few colleagues: "how important is it that the date of a manuscript is more or less correct? ". The date is obviously significant in cataloguing ancient texts, but we face difficulty in giving precise information. Accordingly Andrist illustrates several methods to prepare the online description of a document, like the mono-layer and bi-layer model which also permit one to understand the structure and the content of a codex. 


Information of Parisinus Gr. 1823 in the Gallica Catalogue ( https://gallica.bnf.fr/ark:/12148/btv1b107229706 ) 


The volume ends with an essay in which Joris J. Van Zundert discusses the methodological innovation of digital scholarly editions. Their aim is to collect and present a text in multiple visualizations, in order to illustrate all the necessary information of certain text. That can be achieved also by printed editions, but such tasks as indexing and bibliography are easier to access with the digital medium. Van Zundert also talks about two important capabilities of the digital codex: interactivity and processing. In fact, it is possible to edit, change or annotate a document based on his interpretation, which in a print text can be done only in the margins. The hope is that in the future we could improve the potential offered by the digital revolution and exceed its current limits. As Van Zundert says in conclusion, "Thus we have a glass half full. Good, now let us fill it further ". 

Analysis of Ancient and Medieval Texts and Manuscripts: Digital Approaches is an important source with which to understand how the digital revolution has changed, and also changes, the approach to literary studies. The essays of the volume express, in most cases, the first steps for the future development of collaborations between philology, paleography, codicology and the most recent technologies, with the assumption that many aspects still need to be improved. Even in six years much progress has been made because today classical studies work makes use of digital resources and cannot ignore their contribution to research. 

We can therefore recall some of the most important project of these last years, like the ones developed by Monica Berti at the University of Leipzig: the Digital Fragmenta Historicorum Graecorum (DHFG) and the Digital Athenaeus, which intend to provide a digital version of Fragmenta Historicorum Graecorum (FHG) and Athenaeus’ Deipnosophists and extract from these texts additional information not included in the print edition. A significant database is also Divination and Greek History (Archaic to Classical). Reassessing Greek Oracular Responses, created by Maurizio Giangiulio at the University of Trento: its aim is to edit, comment and translate Delphic literary responses, analyzing their narrative framing and collecting as many loci paralleli as possible. Another Italian project is Online Humanities Scholarship: A Digital Medical Library based on Ancient Texts by the University of Parma, which provides electronic editions of ancient medical sources (papyri, ostraca and tablets), with the possibility to examine texts, translations, commentary, metadata and images. To conclude, it is worth mentioning the project Iliadoscope, developed by David Bouvier and Ariane Jambé at the University of Lausanne: the intent is to visualize the Iliad , its paraphrasis and scholia included in the Byzantine manuscript Genavensis 44, comparing the texts and searching for lexical correspondences. These are only few projects of the last few years, but they demonstrate how the computer-assisted methods are an essential source for humanities and their use will inevitably grow in the future. That is really an achievement. 


# notes

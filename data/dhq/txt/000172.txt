

# Preparatory 


This article seeks to explore concepts of significance to the broader Humanities community, and in so doing graft them into the main trunk of Digital Humanities theory and method. This makes a position statement useful. The argument that follows reflects an approach to Digital Humanities (and a view of Digital Humanities) oriented towards the Humanities Computing tradition and what Patrick Svensson would refer to as the technology as tool approach to the field. Because of this, it risks hypostasizing what is only a thesis into a statement of intent, or worse, a totalizing claim that the ground of our discipline is of a particular (rather than multivariate) nature. That would be an unsupportable, and unproductive, approach. That said, it is my hope that the article provides additional intellectual justification for a turn to code-craft practices outlined at the Speaking in Code workshop at the University of Virginia’s Scholars’ Lab in 2013 , and the adoption of a mindset broadly commensurate with digital artisanship. That topic requires further exploration, and is somewhat peripheral to this present article, but is important to keep in mind: as a scholarly discipline we require conceptual anchors and an openness to theories and methods borrowed from cognate disciplines, but in our daily practice digital humanists are reminded of the importance of craft and the value of tacit knowledge. It is my contention, even accepting my own rudimentary craft skills, that this is where the discipline has the most to offer the broader Humanities community. 

It is counter-productive, however, to ignore the field’s entanglement with postindustrial culture, the rise of technocratic and neoliberal modes of government, and the so-called crisis in the humanities that has seen Humanities disciplines struggle with policy decisions weighted in favor of Science, Technology, Engineering and Mathematics (STEM) disciplines . Although a complex soup, this comprises the post-World War Two cultural, intellectual and technological background to the field: it is an important part of our heritage as a community. And while it is perhaps of more interest to intellectual and cultural historians than digital humanists per se, it provides essential context — even, perhaps, an ontological ground — that is important to parse if we are to understand the purpose and potential of the field. This is especially so if it is accepted that the crisis in the Humanities, whether real or imagined , is related to deeper epistemological problems connected to the relative value of scientific and humanistic modes of knowledge creation. This issue touches on issues of fundamental importance for the scholarly community: while there is none of the sense of revolution that accompanied the culture wars of the latter twentieth century, some scholars are questioning the binary opposition of foundationalist and anti-foundationalist modes of knowledge creation, symbolized most starkly in logical positivism on the one hand and postmodern relativism on the other. This paper follows G.B. Madison , J. Wentzel Van Huyssteen , Paul Healy , Dimitri Ginev , and Mark Bevir in labeling this effort postfoundationalism. 

These writers are searching for a non-defeatist epistemological stance, one that rejects Cartesian foundationalism as unattainable but remains capable of underwriting the truth-value of our interpretations . The issue speaks to a broader impulse to seek methods that are neither foundationalist nor relativist , but still capable of advancing knowledge. In his recent book on the use of the R programming language for literary study, digital humanist Matthew Jockers touched on something similar when he noted that methods described in his book reflect a post-Popperian stance somewhere between strict positivism and strict relativism . Jocker’s mention of post-positivism represents an important statement given its associations with method in the hard and social sciences, and is deserving of further exploration, but it is outside the scope of this article. Post-positivism does indeed hold significant opportunities for digital humanists working with methods derived from or closely associated to the computer and social sciences, but the argument that follows treads a fine line between researchers like Jockers whose work draws them towards methods prominent in those consciously scientific disciplines, and others who are more focused on cultural theory and critique. Its goal is to explore the possibilities inherent in a bridging concept (postfoundationalism) that might work equally well for all sectors of the community. If successful it will be broadly agreeable to both ends of our disciplinary spectrum; if unsuccessful it may well disappoint all parties. 

Because of its engagement with issues of epistemology and method, there is a need to navigate terrain familiar to historians of ideas: on the one hand the argument must avoid the kind of unit-ideas approach, popular with mid-twentieth century historians like A.O. Lovejoy, which enumerates ideas as a record of in-group culture , and on the other it must not assume to comprehend the inside of actors’ minds . These methodological issues are considerable, and the article’s conclusions should certainly be weighed against them, but if comments like Jockers’ reveal a dawning orientation for the field as a whole there are many threads to pull: intellectual, cultural, historical as well as theoretical and methodological. It will be enough if this present article is agreed to be a useful contribution to that process. 

Another problem is, of course, that any analysis of the digital humanities must traverse vast distances. Brett Bobley, CIO and Director of the U.S. National Endowment for the Humanities (NEH) Office of Digital Humanities (ODH), has presented a fabulously broad definition of the field that illustrates this issue well: I use digital humanities as an umbrella term for a number of different activities that surround technology and humanities scholarship. Under the digital humanities rubric, I would include topics like open access to materials, intellectual property rights, tool development, digital libraries, data mining, born-digital preservation, multimedia publication, visualization, GIS, digital reconstruction, study of the impact of technology on numerous fields, technology for teaching and learning, sustainability models, media studies, and many others. This is well and good and, ignoring for the moment people who would strongly disagree with such a broad statement, useful in its Catholicism. But it makes the task of defining the digital humanities difficult . Even if there is no great desire to define a discipline in the traditional sense of the term, the field needs to find intellectual levers that can make sense of a very broad definitional continuum, and explain to stakeholders what DH is, how it is connected to the current difficulties encountered by the humanities, how it is connected to broader postindustrial culture, and how technical DH outputs should be assessed. Without answers to these issues the field is unlikely to gain either high levels of student engagement, or a portion of increasingly competitive funding sources. 

Postfoundationalism is one such intellectual lever, but the centrality of it to this article should not suggest it is universally applicable, or unproblematic as a concept and label: it could perhaps sit under P alongside hundreds of other similar concepts in a Dictionary of DH Terms , but it is enough to hope that it can serve the purposes of this article and provide entry to the conceptual domain I aim to navigate. It will not be useful for all digital humanists, or be applicable to all DH practices. It certainly shouldn’t be accepted uncritically as a unit-idea in the Lovejoyian sense. It is, however, well-suited to an exploration of the epistemological implications of Svensson’s technology as tool approach to the digital humanities, which focuses on building digital outputs ahead of engaging in more traditional humanistic pursuits related to interpretation and critique. Because of this the technology as tool approach tends to produce non-traditional scholarly outputs like web archives, ontologies, data models, and suchlike. Indeed, much of what follows is informed by development of the UC CEISMIC Canterbury Earthquakes Digital Archive and the search to find robust arguments to make technical work like this count as research . 

Justifying non-traditional humanities work to academic administrators can be a challenging task, which isn’t helped by the relative lack of conceptual work across the community. Jan Christoph Meister has suggested that the amount of energy that our community invests into theoretical and methodological critique of its practices and their limitations is still disproportionally low, resulting in a lack of understanding about what a shared methodology for the field might look like . Meister’s comment is perhaps slightly out of date given the recent debates related to defining DH (one of which will be examined in detail later in this paper), and it ignores significant conceptual work undertaken by projects like the United Kingdom’s AHRC funded ICT Methods Network (AHRC 2005 – 2008) and various communities of practice (TEI, manuscript studies, epigraphy, computational linguistics) but his underlying premise remains valid. There is a need to explain in scholarly terms what key concepts carry weight for digital humanists, and how the field and its associated theories, methods, practices, and outputs relate to broader currents in intellectual culture. It is only by positioning the field in the context of broader scholarly discourses and processes driving the contemporary academic knowledge economy — by defining a shared methodology — that a solid claim can be made for long-term institutional investment in it. 

This article therefore articulates a view of the digital humanities that hopes to advance the discipline across broad academic and scholarly contexts. It will succeed in its aims if it is both comprehensible to newcomers and stimulating for experienced practitioners: a bridging effort, but one undertaken with serious intent. It proceeds by isolating a key debate for examination, describing two concepts that go a significant distance to solving issues raised by that debate (but not far enough), and exploring the theoretical writings of a selection of high profile digital humanists. The goal (a non-trivial undertaking) is to illustrate the utility of postfoundationalism as a conceptual tool, its interdependence with postindustrial culture, and the light it sheds on our understanding of what DH is. If successful, the article, rather than making an essentialist claim that Digital Humanities is defined by postfoundational method, will constitute a contribution to the developing digital humanities agenda : A field’s agenda consists of what its practitioners agree ought to be done, a consensus concerning the field’s problems, their order of importance, the means of solving them (the tools of the trade), and perhaps most importantly, what constitutes a solution. Becoming a recognized practitioner means learning the agenda and helping to carry it out. 


# The DH Moment 


In his introduction to Debates in the Digital Humanities , The Digital Humanities Moment, Matthew Gold notes that the practice has arrived amid larger questions concerning the nature and purpose of the university system . As the discipline develops, these questions of cultural and intellectual context are becoming both more relevant and more contested because, as insiders to DH know, although multivariate the conversation returns to the same basic issue again and again, making it difficult to explain (and therefore justify) the field to university management. Some digital humanists view attempts to divide practitioners into two separate camps as futile, but in practice it’s easier to posit a simple binary opposition: if the field orients itself towards text encoding, computer programming, and producing IT products it is presumed to need to align towards the sciences, engineering, and empirically-oriented humanities and social sciences; if it orients itself towards Theory it is presumed to need to align towards literary and cultural studies. The reality is far more complex than that (text encoding is a classic example, where technical issues are inextricably tied to both theory and knowledge context), but it is a level of complexity that’s difficult to convey to senior scholars and peers in cognate disciplines and service areas, so the dichotomy assumes more significance than it deserves. 

There are some indications that the global community is incapable of pointing the way to a workable compromise. Patrik Svensson has suggested that DH is a twenty-first-century humanities project driven by frustration, dissatisfaction, epistemic tension, everyday practice, technological vision, disciplinary challenges, institutional traction, hope, ideals and strong visions — hardly a situation conducive to clear articulations of intellectual purpose and antecedent . Alan Liu has admitted that he fears: the digital humanities are not ready to take up their full responsibility [to reinvigorate the Humanities] because the field does not yet possess an adequate critical awareness of the larger social, economic, and cultural issues at stake. Liu’s argument is that this unpreparedness stems from a general resistance to theorizing the deeper cultural significance of the discipline across the DH community, in favor of building tools, systems and websites, and programming code . Manfred Thaller probably wouldn’t agree with that sentiment, but appears similarly frustrated about the long-standing tension in the digital humanities over whether the discipline has an intellectual agenda or […] constitute[s] an infrastructure . In sanguine voice Willard McCarty has pointed out that …complaints of stagnation and theoretical poverty… have followed the discipline since at least 1962 . It is worth remembering in this context that the older humanities computing tradition was not associated with what could be termed the main currents of late twentieth century intellectual culture. The heated debate surrounding Robert William Fogel and Stanley L. Engerman’s Time on the Cross: The Economics of American Negro Slavery (1974) provides one example of how it could become enmeshed in topical debates (in this case around cliometrics, or the use of quantification in economic history), but humanities computing was not known for its engagement with high-profile intellectual trends. Analysis of the Humanist email seminar, run by Willard McCarty since 1987, backs this up . 

This has changed in recent years as significant numbers of newcomers have joined the community, resulting in sometimes-heated debates between those supporting a traditional humanities computing perspective based on technology as tool, and those supporting the broader definition enabled by the ODH. The recent DH theory debate is a case in point, and the example I have chosen to illustrate the utility of postfoundationalism as a critical tool. While it is only one of several issues that could be explored, it has been chosen here because it exposes a particularly troubling issue. The discussion exposed the fault lines that resulted from the rapid development of the humanities computing tradition into a broader state-sanctioned Digital Humanities field in the United States. While the locus of the debate was centered in the U.S., it resonated around the international community and has particular significance for the field as it expands into new regions of the world. 

The debate’s basic elements are well rehearsed. It took place over the course of two years, and was conducted primarily amongst North American university-based digital humanists. In an influential talk describing digital humanists’ values, given at the CUNY Digital Humanities Initiative in 2010, Tom Scheinfeldt didn’t mention theory, emphasizing coding and Do It Yourself (DIY) instead . At the time his emphasis seemed unproblematic. At the 2011 MLA conference in Los Angeles, however, Stephen Ramsay put a finer point on the issue by asking: Do you have to know how to code? I’m a tenured professor of digital humanities and I say yes. He followed up by opining: But if you are not making anything, you are not — in my less-than-three-minute opinion — a digital humanist. Ramsay later softened his position, and has produced his own significant contribution to digital literary theory, but his comment brought a challenging vein of digital humanities discourse into the light of day. Some people were angered, and felt that such a brazen attitude opened up a space for them to air mounting grievances. The assumption was that Ramsay’s comment betrayed a lingering prejudice across the discipline that equated the ability to write computer code with hostility to Theory. This may or may not have been an unjust conflation, but for whatever reason the association of coding with anti-theoretical prejudice had become a touchstone issue. 

It’s easy for people outside the United States to forget that Alan Liu delivered a paper at not only the same MLA conference as Ramsay, but also the same panel . Titled Where is Cultural Criticism in the Digital Humanities? , it suggested that this cherished focus on more hack, less yack (focusing on text encoding and computer programming at the expense of theory and cultural criticism) threatened to throw away a crucial opportunity for digital humanists, their students, and the wider tradition. Later in 2011 and the early months of 2012 the topic yielded one of the richest intellectual debates outside Humanist in the history of the discipline. The debate was prompted by a blog post by a young American scholar, Natalia Cecire, who disagreed with the zero-sum logic that it [an emphasis on coding] implies . Primed by a year’s worth of discussion resulting from Ramsay and Liu’s comments at the MLA in January, a slew of posts and tweets on the topic dominated digital humanities discourse for several weeks, before petering out with a return to the status quo: hack over yack. Most digital humanists, it seems, agreed with Tom Scheinfeldt, who tweeted that DH arguments are encoded in code. I disagree with the notion that those arguments must be translated / re-encoded in text . 

Perhaps in an acceptance that the time had come to provide a scholarly forum for the debate, the first issue of the partially crowd-sourced Journal of Digital Humanities , produced by George Mason University’s Roy Rosenzweig Centre for History and New Media in late 2011, was devoted to the Theory problem. Natalia Cecire was invited to provide an introduction, where she claimed that the hack versus yack divide had sundered the connection between saying and doing. Cecire claimed that hacking represented a dominant discourse across the discipline that celebrated tacit knowledge and valued only embodied, experiential, extradiscursive epistemology” at the expense of deeper philosophical, ethical, and economic issues . Although a far less emotive (and important) topic, it wasn’t unlike the accusations directed at cliometricians like Fogel in the previous century: a claim that positivism, and especially scientism masquerading as post-positivism, becomes anathema to the Humanities when it excludes more traditional methods. In her piece, Jean Bauer openly stated that she was insulted by these kinds of comments, which she felt betrayed a lack of understanding about the design decisions required for DH products . Several other contributions were similarly forthright in their defense of tacit knowledge. Fred Gibbs came closest to mediating a way forward by simply pointing out that [p]art of what defines a discipline is the rhetoric and aesthetics of its scholarly discourse and there are very real practical needs for the development of DH-specific discourse, in order to evaluate scholarly outputs and engage in other normal administrative tasks . Perhaps the most effective contribution (and certainly the most concise) to that first issue of the Journal of Digital Humanities was offered in Word and Code , jointly authored by Tom Scheinfeldt and Ryan Shaw. It consisted merely of Scheinfeldt’s tweet and Shaw’s reply: 
DH arguments are encoded in code. I disagree with the notion that those arguments must be translated / re-encoded in text. 

@foundhistory   @ncecire  If you can't explain to me in words how your code works, you don't really know how it works. 
The two tweets provided a summation of the hack versus yack debate in 280 characters, with a substantial dose of irony. 

What’s missing here, crucially, is that the theory debate prompted by Cecire probably only represented the further development within the U.S. digital humanities movement (itself containing digital history, digital literary studies etc.) of digital cultural studies, an event that had been presaged by the pre-existence of digital media studies , and should have been welcomed as a sophisticated addition to the field. The fact that it wasn’t points to one of the fundamental weaknesses in the movement: consistent recourse to a category error that conflates the contributing fields of the digital humanities with the (extra)discipline itself. The problem usually appears with the conflation of DH with digital literary studies, rather than digital cultural studies, to the point where it sometimes seems as if English departments are taking over the field to the detriment of digital history, classics and so on. Mathew G. Kirshenbaum devoted an essay to the issue in 2010. Titled What Is Digital humanities and What’s It Doing in English Departments?, the piece noted that English departments have historically been hospitable settings for scholars interested in humanities computing, because of their natural interest in text analysis and publishing . Kirschenbaum was quite right in noting this can only be a positive thing given the need for institutional support, but DH has also been well supported in History and Classics departments and it would be improper (and perhaps even absurd) for English departments to claim special ownership of the field. If this was to happen there would not only be confusion about what DH is, but digital historians and classists (etc.) might become unwittingly mired in the famously heated debates characteristic of literary studies. 

This was precisely what happened when Stanley Fish , , and later Stephen Marche wrote essays objecting to the digital humanities in the New York Times and LA Times respectively. It could be that they were hoping to prompt a campaign of apologia pro vita sua against DH, as Anthony Daniels has admitted to savoring , but they were ill-served by their sources. Both writers, after presumably cursory research into the digital humanities, assumed that digital humanities equated to digital literary criticism and proceeded to damn the entire subject-area for the (perceived) sins of this one contributing area. Their argument against digital humanities revolved around the lack of benefit in quantitative text analysis rather than the utility of historical GIS, concordances of ancient texts, digital variorums, or transcriptions of philosophical writings. If people involved in digital literary studies were perplexed at the hostility, digital humanists from other fields were left wondering why eminent literary scholars were damning their field without seeming to know their specific area of it even existed. A cursory glance at the book of abstracts for the primary ADHO conference would have alerted Fish et al. to their mistake. In many ways it was an embarrassment for American literary studies, as some of their finest betrayed a tendency to engage in heated polemics in ignorance of elementary facts. The problem continued at the 2013 MLA conference, with one panel discussion, titled The Dark Side of the Digital Humanities generating ire for its participants’ conflation of DH with the recent trends towards Massive Open Online Courses, or MOOCS, which digital humanists have been largely uninterested in (the topic is of more interest to people interested in eLearning) . 

Category errors like these, perhaps better described through reference to the parable of the blind men and the elephant, do indeed speak to an under-theorized discipline: one that undermines external perceptions and internal cohesion, and suggests that the field doesn’t quite know what it is. Given he has watched digital humanists debating the same issue for several decades it’s telling that Willard McCarty wondered in 2012 whether debates like these indicate …immaturity and lack of outward reach… characteristic of the discipline as a whole. To some observers, and despite a large number of blog posts and even a THATCamp unconference devoted to the topic , it seems to some observers as if the discipline is stuck in a Becktian moment …where the advocates of computation and interpretation are locked in a dichotomous opposition . 

And yet Scheinfeldt’s tweet points to a way out of the situation. In distilling decades of debate into 140 characters it presents us with a nicely reductive place to initiate analysis. Contra Cecire’s claim that it amounts to zero-sum logic or a felix culpa , the implication I draw is that fundamental disciplinary truths must reside inside this tautology: DH arguments are encoded in code. I disagree with the notion that those arguments must be translated / re-encoded in text. Wittgenstein might claim Scheinfeldt’s comment simultaneously says nothing and opens up a whole world of interpretation. The key, of course, is finding the right tools to explore this strange new world. Ultimately, of course, this article will argue that postfoundationalism is one of the most satisfactory tools we have at our disposal to understand it, especially when considered in the context of postindustrial culture. In order to understand why, we must explore two slightly less satisfactory concepts first. 


# Immanence 


It is a fruitless exercise to imply that the digital humanities are incapable of leading us towards intellectual depths as American literary critics like Fish and Marche have, or that the entire field should adopt the theoretical perspectives of one of its contributing disciplines (or, indeed, the theoretical debates of one of its contributing countries). The knowledge domain is too different from anything we’ve encountered before, and study of it too limited, for us to understand what its intellectual potential might be, let alone decide today what its central preoccupations should be for years into the future: it is, and has been for several decades, in development. This paper suggests that, given examples like that outlined above, digital humanists need to continue developing a set of conceptual tools capable of exploring, in the first instance at least, the hackers’ tautological stance towards code. 

It’s easy enough to see why digital humanists have been having the same conversation for decades. Despite the fact that it brings us up against some difficult hermeneutic issues, code is at once our tool, our historical record, and the basis of our theoretical canon. We cannot get away from it. By extension, an understanding of code (the precise level of understanding is yet to be defined) must be, along with knowledge of the humanities themselves, a sine qua non of entry to the field. Louis Menand would no doubt suggest that we are isolating this aspect of our practice as a means of exceeding [our] own history, as a way of defining borders and laying claim to long-term existence within the academy in the same way that historians, literature professors and lawyers did before us . The question isn’t so much whether we are going to make such a stand, and what we’re going to make it over, as to whether we have the critical tools to make the stand meaningful to our peers in neighboring disciplines, and administrators requesting justification for continued support. In order to do this we need to start with simple concepts and move outwards: first immanence , then the epistemology of building. 

For the hacking branch of the DH community, then, which appears to me to be the obviously (and appropriately) dominant branch of the discipline, computer code is immanent. The Software Studies and Critical Code communities (themselves part of the DH community in the broader definition offered by Brett Bobley) have done considerable work on this subject ; ; ; ; , but in simple terms it is easy to illustrate what this means. One of the signal DH publications of 2011 used facial recognition to recover the public identities of thousands of nineteenth-century Chinese Australian immigrants previously hidden in a massive archival stack at the National Archives of Australia. Tim Sherratt’s single webpage, titled the real face of white Australia , is self-explanatory. The real interest for digital humanists was in his accompanying explanatory blog post , and the two Python scripts he posted on his public Github account . 


# Script 1. extract_faces.py 

#!/usr/bin/python # Loop through images and feed to facial detection script import os import face_detect #rootdir = "/home/tim/mycode/recordsearch/src/recordsearchtools/files/E752" rootdir = "/home/tim/mycode/recordsearch/src/recordsearchtools/files/ST84-1" #rootdir = "/home/tim/mycode/recordsearch/src/recordsearchtools/files/test" #rootdir = "/home/tim/mycode/recordsearch/src/recordsearchtools/files/ST84-1/1907-391-400-[1731871]" for root, dirs, files in os.walk(rootdir, topdown=True): for file in files: print 'Processing %s' % file face_detect.process_image(os.path.join(root, file)) 

# Script 2. face_detect.py 

#!/usr/bin/python # face_detect.py # Face Detection using OpenCV. Based on script at: # http://creatingwithcode.com/howto/face-detection-in-static-images-with-python/ # Usage: python face_detect.py [image filename] import sys,os from opencv.cv import * from opencv.highgui import * from PIL import Image, ImageOps CLASSIFIER = '/usr/share/doc/opencv-doc/examples/haarcascades/haarcascade_frontalface_default.xml' CROP_DIR = '/home/tim/mycode/recordsearch/src/recordsearchtools/files/crops' def detect_objects(fn, image): """Detects faces and then crops the image.""" #grayscale = cvCreateImage(cvSize(image.width, image.height), 8, 1) #cvCvtColor(image, grayscale, CV_BGR2GRAY) storage = cvCreateMemStorage(0) cvClearMemStorage(storage) #cvEqualizeHist(grayscale, grayscale) cascade = cvLoadHaarClassifierCascade(CLASSIFIER, cvSize(1,1)) faces = cvHaarDetectObjects(image, cascade, storage, 1.3, 3, CV_HAAR_DO_CANNY_PRUNING, cvSize(20,20)) if faces: i = 1 for f in faces: #newfn = fn + ".output.jpg" #os.system("convert %s -stroke red -fill none -draw 'rectangle %d,%d %d,%d' %s" % (fn, f.x, f.y, f.x+f.width, f.y+f.height, newfn)) #os.system("mv %s %s.orig" % (fn, fn)) #os.system("mv %s %s" % (newfn, fn)) #print("[(%d,%d) -> (%d,%d)]" % (f.x, f.y, f.x+f.width, f.y+f.height)) file, ext = os.path.splitext(fn) im = Image.open(fn) # Increase selected area by 50px on each side then crop im = im.crop((f.x-50, f.y-50, f.x+f.width+50, f.y+f.height+50)) # Minor contrast adjustment im = ImageOps.autocontrast(im, cutoff=0.5) im.load() crop = '%s/%s_crop_%s.jpg' % (CROP_DIR, os.path.basename(file), i) im.save(crop, "JPEG") check_crop(crop) i += 1 def check_crop(crop): """Try to reduce false positives by doing a second pass and deleting images that fail.""" image = cvLoadImage(crop); storage = cvCreateMemStorage(0) cvClearMemStorage(storage) cascade = cvLoadHaarClassifierCascade(CLASSIFIER, cvSize(1,1)) faces = cvHaarDetectObjects(image, cascade, storage, 1.3, 3, CV_HAAR_DO_CANNY_PRUNING, cvSize(20,20)) if faces: if faces[0] is None: os.remove(crop) else: os.remove(crop) def process_image(fn): image = cvLoadImage(fn); detect_objects(fn, image) def main(): image = cvLoadImage(sys.argv[1]); detect_objects(sys.argv[1], image) if __name__ == "__main__": main() 
For a DH hacker these scripts are rich in humanist detail, from the cultural phenomenon that is Github, to the problems posed by finding and visualizing sources within very large datasets, to the open source code movement that underpins DH and prompted Sherratt to post the code in an online forum, to the referencing of the scripts he used (not only due to licensing requirements but to show his colleagues how easy his task was, using code from http://opencv.org/ and adapting it to his purpose). This isn’t to mention the background to the Python scripting language, its suitability for entry-level programming and its widespread adoption and centrality to early 21 st century digital products. And like coffee stains in a book margin, we find in the extract_faces.py configuration script references to "/home/tim/mycode , suggesting the work was done not on a proprietary Windows or OSX operating system, but on an open source Linux machine. If the term code is read in even broader terms than this, as a metaphor for a thorough-going understanding of how digital culture is engineered (data architecture and packetization, the nature of databases and networks, the nature of programming languages etc.) it is possible to see even more possibilities, at the point where DH intersects with not only computer science, but the History of Technology, Software Studies, and Science and Technology Studies (STS). In 10 PRINT CHR$(205.5+RND(1)); : GOTO 10 Nick Montfort et al. point out that attempts like these, to position the program’s output in a space of symbolic meanings and design principles… offer rich new interpretative possibilities, but it is equally important to recognize that at some level source code will always remain impenetrable, or spectral . Such is the complexity of modern computing that it has become impossible for a single person — even the author (in the event there is a single author) — to grasp what’s going on as functions are called, libraries unpacked, and voltage levels set within logic gates. The reading above ignores these issues and the mechanics of the code itself, but it perhaps indicates some of the mountainous interpretative possibilities inherent in the lines of code being created by digital humanists. 


# The Epistemology of Building 


While the immanence of computer code provides justification to scholars involved in Software Studies, and digital humanists aware that field is either cognate or part of the broader tent depending on their definitional stance, it doesn’t provide a lot of justification for digital humanists seeking evidence that the more technical kinds of digital humanities outputs should be accepted for scholarly assessment. It either relies on techniques of source criticism practiced by a range of humanist disciplines, or merely states the obvious point that people who can write code can also read and interpret it. In order to justify technical outputs as worthy of assessment digital humanists would be more advised to refer to an approach once outlined by Willard McCarty in Humanities Computing (2005), augmented by comments in Humanist and journal articles . 

McCarty’s comments may well represent first use, but my purpose here is not to establish an eponymous origin. My purpose is to suggest that his cognitive stance has become so widespread it represents a habit of mind or, mentalité , that reflects the goals and aspirations of a significant portion of the community, including Franco Moretti (2005), Julia Flanders (2009; 2012), Galey and Ruecker (2010), Ramsay and Rockwell (2012) and several others. The implications of the stance are fascinating. In extended commentaries later encapsulated in Scheinfeldt’s epigrammatic tweet, McCarty suggests that theories of computer coding, modeling and design are capable of providing an epistemological basis for the digital humanities; that rather than being mere by-products of the development process, they contain arguments that advance knowledge about the world . The argument proffered is that the need to create models of reality (ontologies, database schemas, algorithms and so on), required to allow computers to mathematically parse problems posed by their human operators, offers a radical new methodological basis for future humanities research. Rather than being merely an indication of computers’ inability to accommodate the complexities of human thought and emotion, and by extension historical reality, the suggestion is that the very inadequacies of the models — their propensity to be radically inadequate, or at best only broadly reliable — suggest a new way of looking at the world that is at once accepting of failure, more in tune with scientific method, and oriented towards process instead of Truth: Computational form, which accepts only that which can be told with programmatic explicitness and precision, is thus radically inadequate for representing the full range of knowledge — hence useful for locating what gets lost when we try to specify the unspecifiable. Ramsay and Rockwell interpret this to mean that we need to develop a humanistically informed theory of the making of technology, an epistemology of building that provides scholarly justification for DH outputs in a way that makes sense to our peers in cognate disciplines . 

Although there are notable exceptions, such as McCarty’s modeling of Ovid’s Metamorphosis , the problem with attempts to define an epistemology of building is that they threaten to float free of the broader humanities tradition. Commentators like Liu and Cecire might argue that, as Allen Tate said of the American Southern Critics, they are locked in the present and cut off from the benefit of the fund of traditional wisdom that has dealt with similar problems before. Without arguing specifically for more critical or cultural theory, David Berry suggests something similar when he comments that digital humanists need to problematize what Lakatos (1980) would have called the hard-core of the humanities, the unspoken assumptions and ontological foundations which support the ‘normal’ research that humanities scholars undertake on an everyday basis. Following this line of argument, which is a powerful one, arguments for more or less critical or cultural theory are simply components of a larger problem. The hack versus yack debate means little in the context of a 2000-year-old tradition, after all. This isn’t to criticize the many excellent scholars who have contributed to the discipline over the decades, or to ignore the growing body of work (much of it cited in this article) that suggests growth towards what Lakatos terms the ‘hard-core’ humanities. Any digital humanist who has spent a considerable amount of time staring at code to work out a particular problem will understand why this is easier said than done: the conceptual divide that separates Computer Science and the Humanities is large, and it is natural to only think in one of the two paradigms at any one time. Digital humanists need bridging concepts, or concepts that work just as well for the digital humanities as their analog cousins — levers capable of raising our conceptual understanding to new levels. 


# Postfoundationalism 


Postfoundationalism holds promise as one of these levers. Although it is only one of several that will be required, it offers our nascent epistemology of building a useful tool. Mark Bevir points out that for historians postfoundationalism has the great benefit of avoiding the simplistic anchoring of explanation in pre-determined facts as with modernist discourse, or the dissolution of fact into fiction characteristic of postmodernism. Postfoundationalism asserts that there is no point asserting either more confidence in our understanding of reality than is justified (as with modernism and logical empiricism) or retreating into a pessimistic view of our ability to grasp any one reality at all (as with postmodernism and postmodern deconstruction) . Rather, in a claim that could perhaps be criticized for claiming to have cut the Gordian knot, postfoundationalism reject[s] the possibility of facts outside theoretical contexts. All knowledge incorporates both facts and theories . It is an intellectual position that balances a distrust of grand narrative with an acceptance that methods honed over centuries and supported by independently verified evidence can lead, if not to Truth itself, then closer to it than we were before. Philosopher of science Dimitri Ginev notes that postfoundationalism offers both a hermeneutic ontology of existence and a hermeneutic theory of historicity, and buttresses itself by asserting the validity of both the research process and the outputs of that research process . Revealingly, Paul Healy suggests that postfoundational rationality aims to develop a situated learning process conducive to the accretion of knowledge, rather than either positivist or relativist idealisations . Its devotees hope that it is capable of moving human knowledge and understanding beyond the just in time, conflicted postmodern historicism described by Alan Liu towards something equally accepting of complexity, but more optimistic and empirically oriented. Healy neatly summarizes the resulting communicative domain in terms of disciplines embracing a robust context-transcendent truth standard which, in virtue of preserving the ‘aporetic tension’ inherent in the distinction between what is true and what we hold to be true, suffices to ensure that proffered knowledge claims are held open to critical scrutiny in an indefinitely extended array of situated forums. 

The approach would seem to be well suited to a (trans)discipline like Digital Humanities that runs the gamut from empirically oriented text analysis to the development of database models and cultural critique. It could well be, because it is a field so reliant on a similar hermeneutic, that the digital humanities prove important to the development of postfoundationalism in the coming decades. In straddling the humanities and computer science, and using methods derived from fields as diverse as computational linguistics and sociology, its practitioners are confronted with a need to re-envisage the nature and goals of humanistic enquiry and method, in order to make sense of the radically different questions they’re being confronted with. It makes sense to seek out different new approaches to assist them in that task. Although it is essential that the discipline looks to fields like computer science (and perhaps mathematics and logic) before reinventing the wheel, the basic assumption must surely be that a new problem domain is likely to require new theories as well as new methods. And the parallels between emerging DH theory and postfoundationalism are easy to illustrate. 

Few statements of method could suggest a postfoundational orientation better than McCarty’s comment that computational models, however finely perfect, are better understood as temporary states in a process of coming to know rather than fixed structures of knowledge , or his later point that the word computing is a participle — a verbal injunctive that turns things into algorithmic performances , requiring attention to an ongoing process of iterative modeling rather than final outcomes. By meditating on the procedures involved in the production of electronic texts, from the inscription of bits onto hard-drive platters, to their abstraction in machine code, assembly language and higher level programming languages, to presentation on our screens, Matthew G. Kirschenbaum offers what could be seen as a postfoundationalist argument for critical exegesis focused on process, propagation, and becoming, rather than the fixity of texts, screen, or image: New media cannot be studied apart from individual instances of inscription, object, and code as they propagate on, across, and through storage devices, operating systems, software environments, and network protocols … Such an interpretation works equally well for a writer like Stephen Ramsay, who tries to locate a hermeneutics at the boundary between mechanism and theory , pointing out that [T]he stratum that we lodge ourselves upon with algorithmic criticism is one in which both results and the textual generation of results are systematically manipulated and transformed, connected and reconnected with unlike things. Peter Lunenfeld et al. suggest that the discipline needs to engage with design as a method of thinking-through-practice : Digital Humanities is a production-based endeavor in which theoretical issues get tested in the design of implementations, and implementations are loci of theoretical reflection and elaboration. Franco Moretti offers something similar when he notes that for him [The map itself is not an] explanation, of course: but at least, it offers a model of the narrative universe which rearranges its components in a non-trivial way, and may bring some hidden patterns to the surface. Extending the theme into the materialist world of the hard drive platters and inscribed bits that mediate the manipulation of digitized sources, William Turkel suggests digital humanists would be well-served to think in terms of transduction , the conversion of energy from one form to another . In their introduction to the Journal of Digital Humanities special issue devoted to topic modeling, Elijah Meeks and Scott Weingart note that: [I]n digital humanities research we use tools, make tools, and theorize tools not because we are all information scientists, but because tools are the formal instantiation of methods. 

It would presumably be possible to find more examples, and others that adopt a quite different stance, but in this article it is only my intention to draw attention to a broad habit of mind or mentalité — enough to justify further exploration — not to assert that postfoundationalism should represent the definitional sine qua non for the field. That argument could perhaps be attempted, but it would require a book-length study and even then be difficult to avoid regression into a totalizing discourse. The point is more that the statements above suggest a broadly accepted vision of interpretation as praxis as much as practice, engagement with a process of continuous methodological and, yes, theoretical refinement that produces research outputs as snapshots of an ongoing activity rather than the culmination of completed research. Postfoundationalism offers a way to package these impulses together momentarily, in order to consider their collective implications. 

There is something in postfoundationalism (in its secular articulation at least), which resonates with the epistemological stance adopted by a range of digital humanists. Scheinfeldt’s tweet suggesting the immanence of code, Sherratt’s webpage, McCarty’s models, Kirschenbaum’s digital forensics, Ramsay and Rockwell’s epistemology of building, Moretti’s maps, Ramsay’s algorithmic criticism, Turkel’s transduction , Meeks and Weingart’s topic models — these are all examples that sit nicely with the postfoundational stance because they speak to the immanence of knowledge, the significance of its built nature, its coherence not within external facts or contrived narratives but within webs of structure and meaning driven forward by an iterative process, or praxis, of constant becoming. Research methods thus come to include the development of ontologies, schemas, authority control systems, algorithms, scripts, websites, databases and other digital tools that act as grist to an ongoing dialectic between reality, representation, and understanding. Any supporting theoretical corpus would focus on the principles and critical tools that sharpen and refine those methods. 

This approach has significant implications for the broader humanities tradition. By rejecting certain kinds of digital output as being alien to the humanities, or simply not valid research, is to cut short a process that has the ability to provide deep insights into our human world, and to adopt a depressingly short-sighted and pessimistic view of the opportunities provided by digital tools and methods. Opposition to them betrays an essentialist understanding of what kinds of knowledge and meaning-production are valid, in much the same way that scientistic claims function . It is this kind of logic which has led the University of Canterbury Library to accept the deposition of the ontology for the UC CEISMIC Digital Archive into its institutional repository. Although it is by no means a normal output for a humanities research team, it is accepted that it is a both a contribution to knowledge, and one that scholars need to be able to reference and critique. 


# Postindustrial Culture 


Postfoundationalism appears to be a common epistemological stance amongst digital humanists, and a useful critical tool to help communicate to administrators the scholarly value of technical digital humanities outputs, but it can also help explore and communicate the relationship of the digital humanities to wider postindustrial culture. In doing so it helps explain the less technical, more politically and theoretically oriented, instantiations of the field, making it at once a powerful explanatory tool (it can help define both narrow and broad definitions of the field) and a potentially dangerous one (it could be latched upon as the rather than a way to define the field). 

The political implications of digital humanists’ postfoundational orientation can be seen in the development of the #alt-ac, or alternative academic career movement. #alt-ac began when Brian Croxall, a digital humanist and adjunct faculty member at Emory University, had a colleague deliver a paper for him at the 2009 MLA in absentia because he couldn’t afford to attend. Titled The Absent Presence: Today’s Faculty , the paper outlined the difficult job search process Croxall had been through, and the unfairness implicit in the American university system, which positions adjunct faculty as the waste product of graduate education . The paper and accompanying blog post generated a storm of interest on Twitter and prompted a conversation about alternative academic careers on that service between Bethany Nowviskie of the University of Virginia’s DH Scholar’s Lab and Jason Rhody from the National Endowment for the Humanities. They proposed the #alt-ac hashtag be used to capture conversations about alternative academic careers, and Nowviskie later established #Alt-Academy, an online collection of essays about the subject. The digital humanities community, though their use of Twitter, had prompted a significant protest against an unfair academic system, and given voice to a large body of disenfranchised but important stakeholders. The #alt-ac community continues to educate and advocate for change in both the orientation of graduate students’ job searches, and the attitude of American universities to adjunct faculty. 

It needs to be remembered that Twitter was by no means an accepted part of academic culture in 2009; indeed, its appearance was met with similar concern about the end of intellectual life as accompanied Wikipedia earlier in the decade. Its use was being spear-headed by digital humanists who, rather than seeing it as a threat to intellectual integrity and the ability of people to read extended passages of prose, chose to view it as merely another useful technology that could be used as part of their broader process: the service was used by digital humanists to offset their geographic dispersal, share ideas and new tools, and build an online community. In 2010, Tom Scheinfeldt went so far as to point out that the use of services like Twitter went beyond the quotidian, and were actually contributing to the development of a radically new mindset: In as much as digital humanities is an Internet-based social network, it should come as no surprise that digital humanities looks a lot like the Internet itself. Digital humanities takes more than tools from the Internet. It works like the Internet. It takes its values from the Internet. Digital Humanities is not only characterized by the use of tools like Twitter (along with code, databases, ontologies etc.), it is constituted by them; the discipline assimilates digital tools and methods to the point where they become the thing itself. This is, of course, exactly what we should expect in bringing technology into such a fundamental relationship to scholarly activity. As Heidegger noted in 1949, technology is more than mere techne , or practical art: the manufacture and utilization of equipment, tools, and machines, the manufactured and used things themselves, and the needs and ends that they serve, all belong to what technology is. Technology itself is a contrivance — in Latin, an instrumentum . As Galey and Ruecker noted in their contribution to the DH discussion about the epistemology of building, tools like Twitter ‘contain arguments that advance knowledge about the world’. In assimilating them into fundamental humanistic practice, to the point where understanding of their essential (engineered) nature is a requirement of participation in the debate, digital humanists are engaging in a postfoundational process with far-reaching consequences. A similar argument can be made for the fringe DH activity of contributing to post-disaster cultural heritage archiving and recovery, expressed in projects like the September 11 Digital Archive , the Hurricane Digital Memory Bank , the UC CEISMIC Canterbury Earthquakes Digital Archive , and Our Marathon: The Boston Bombing Digital Archive . Although not the first thing people would expect humanists to become involved in, it doesn’t take much thought to realize that the interventions of these teams was informed by a range of humanist thinking — about civic responsibility, the importance of cultural memory, public history, engaged scholarship — and that were it not for them significant amounts of valuable cultural heritage content would have been lost. The teams’ scholarly knowledge was put to use via postfoundational methods that resulted in significant contributions to national and international culture. 

As disaster archiving and the development of the #alt-ac community suggests, a growth in postfoundational method has developed coextensively with digital humanities discourse entering the broader public domain. In the American context it might even be reasonable to suggest the growth in postfoundational method has resulted in an increase in participation in the public domain. Prof Hacker , a blog devoted to trends in higher education and technology started after a THATCamp unconference, was hosted by the Chronicle of Higher Education in September of 2009. At about the same time articles about both the digital humanities and the general state of graduate education began to appear in trade and IT publications. Articles about the 2011 MLA published in The Chronicle of Higher Education on subsequent days heightened interest. William Pannapacker’s Digital Humanities Triumphant? described the intense interest in digital humanities sessions at that conference, prompting Fish’s series in the New York Times in response . Jennifer Howard focused her attention on the digital humanities and #alt-ac movements, exploring the connections between them . At the same conference, Alan Liu announced the publication of 4Humanities , a web community designed to harness the interest for the benefit of the humanities. 4Humanities is overt about its role as critic and conscience of contemporary culture, with a special focus on the effect postindustrial culture is having on the arts and humanities. The connection between DH and advocacy is made explicit on the Mission page: 4Humanities began because the digital humanities community — which specializes in making creative use of digital technology to advance humanities research and teaching as well as to think about the basic nature of the new media and technologies — woke up to its special potential and responsibility to assist humanities advocacy.  The digital humanities are increasingly integrated in the humanities at large.  They catch the eye of administrators and funding agencies who otherwise dismiss the humanities as yesterday’s news.  They connect across disciplines with science and engineering fields.  They have the potential to use new technologies to help the humanities communicate with, and adapt to, contemporary society. These projects have been added to by The Praxis Network , a group of allied but differently-inflected humanities education initiatives… engaged in rethinking pedagogy and campus partnerships in relation to the digital and ADHO Special Interest Groups like Global Outlook: Digital Humanities (ADHO, 2013-), which seeks to assist in the equitable global development of digital humanities as a field. 

Transform DH and Postcolonial Digital Humanities occupy more overtly theoretical territory, using critical theory to deconstruct the white, middle-class, and straight nature of the Digital Humanities and press for more inclusive attitudes. These projects deploy critical theory in opposition to what they feel is blindness within the discipline to significant inequities across racial, gender, class and sexual boundaries, but pay close attention to the built layers of technologies. Their attitude was summed up at a paper delivered at Digital Humanities 2013, titled Digital Humanities: Egalitarian or the New Elite? , where a variety of speakers outlined the work — technical, theoretical, and political — required to ensure the community’s claims of inclusiveness are supported by actions, technical standards and protocols . 

Just as Software Studies and Critical Code brushed up against my discussion of immanence, so this part of the digital humanities brushes up against projects like the Humanities, Arts, Science and Technology Alliance Collaboratory and the Fembot Collective , and the vast terrain explored in Science and Technology Studies, Cultural Studies and New Media. This isn’t to mention the long-standing connection between the digital humanities and the open access movement, symbolized in projects like Open Humanities Press and Press Forward but reaching far deeper, into relationships with leading digital presses at M.I.T. and Michigan University, and connections into library science and scholarly communication networks like Media Commons: A Digital Scholarly Network . It would no doubt be possible to continue surfacing relevant examples, but it is enough to note that over the course of a decade, from around 2001–2011, the digital humanities have moved from an emphasis on technique (represented in the humanities computing tradition) to a blended extra-disciplinary praxis involving a continuum that ranges from purely technical work to new media and political advocacy. This process has caused significant and understandable tension; the stakeholder community has struggled to deploy conceptual tools capable of accommodating such a rapidly expanding audience. 

My feeling is that this tension stems from discomfort at the extension of the field towards cognate disciplines that are often deeply critical of the very technologies digital humanists rely on. The critical pressure that has been applied through exposure to these disciplines has resulted in the culturally and critically engaged projects outlined above, which indicate an acceptance of the field’s complex relationship to postindustrial culture. This is unsurprising given the backgrounds of some of the scholars involved in them. Before establishing 4Humanities Alan Liu explored the relationship between the Humanities and postindustrial culture in The Laws of Cool: Knowledge Work and the Culture of Information , arguing that postindustrialism is a technological rationality that has led to the usurpation of the knowledge economy by corporate interests and threatens …the death of knowledge in the information age . He argued that this has shifted the context for the humanities into corporate environments, requiring new analytical techniques, and new research agendas focused on the analysis of corporate culture and power. 4Humanities reflects a response to this insight, actively countering myths benefitting postindustrial technocrats with evidence from within the Humanities themselves, but anxieties remain. 

At the 2013 MLA Wendy Hui Kyong Chun delivered a talk titled The Dark Side of the Digital Humanities, which pointed out the paradox of a supposedly booming new discipline producing insecure jobs, lacking scholarly recognition, and requiring a constant search for funding. Chun claimed she wasn’t criticizing the Digital Humanities so much as the general euphoria surrounding technology and education and the vapid embrace of the digital that feeds into the corporatism critiqued by Liu. Her compelling argument was that the heightened interest in the digital humanities (the development of a bandwagon ) …allows us to believe that the problem facing our students and our profession is a lack of technical savvy rather than an economic system that undermines the future of our students . Chun’s talk provided a moment of stuplimity, a word coined by Sianne Ngai to refer to those peculiarly modern moments when wonderment gives way to an extended duration of consecutive fatigues : As in the case of the repeated pratfalls of the slapstick comedian, stuplimity emerges in the performance of such fatigue-inducing strategies, in which the gradual accumulation of error often leads to the repetition of a refrain: too strong ; or something wrong there. Chun was articulating specific concerns, but they were informed by an awareness of the kind of cultural critique practiced by Liu. She was pointing out that the hype associated with the digital humanities shouldn’t (or shouldn’t be allowed to) hide the fact that the field is as pressured as any other in the arts and humanities by technocratic tendencies and a drift towards corporatism within universities. 

The field’s attitude to this situation — being beset by the same pressures that threaten the broader tradition — could define its future. Rejecting the insights offered by political and cultural theory risks complicity with troubling aspects of the contemporary world ; ; . [S]unny prognostications about a technologically-enabled future of emancipated knowledge workers and efficient markets have been undermined not only by the growth of large multinational technology companies, but unfair labor practices that have all too familiar parallels in the nineteenth and twentieth centuries . As Foucault pointed out, technical forms of knowledge not only influence the organization of corporate culture, but social life and norms of conduct as well, making cognizance of the issues even more pressing . In another piece of writing Chun goes so far as to suggest we have entered a period where long-standing enlightenment definitions of personal identity and governmentality have been radically altered . 

Experienced digital humanists are aware of the issues. As Julia Flanders puts it, [d]igital humanities projects take place, strikingly, in a universe constrained by a set of technical norms that govern the informational and operational behavior of the digital environment . Just as a builder needs her tools, then, so she needs access to ethical guidelines and informed design histories to avoid contributing to (or merely providing more reason to resist) the negative social and economic effects of contemporary technology. This brings to mind the work of writers like Chun ; and Anne Balsamo , who consider the intersection between software, design philosophy, identity, and engineering. These writers offer crucial insights into the digital age, and need to be included within digital humanists’ worldview so they can provide input into postfoundational methods. The same can be said for the many authors who have produced books and articles on the history of computing, and historians of technology generally. This is a very well established field, served by an excellent professional body (the Society for the History of Technology, or SHOT, established in 1958). Efforts should be made to understand where synergies between it and DH lie, especially regarding the concept of materiality and the socio-cultural impact of digital technologies, but also in relation to pedagogy and research goals. 

If these insights are grafted onto the turn to code-craft signaled at the NEH funded “Speaking in Code” workshop in November 2013, the field will have a powerful toolkit at its disposal. Although nascent, this craft movement holds great promise as a means of expressing postfoundational methods through a guild or trade-based approach that emphasizes building and experimentation. The stated goal of the workshop can be described as postfoundational in its desire to give voice to what is almost always tacitly expressed in our work: expert knowledge about the intellectual and interpretive dimensions of DH code-craft, and unspoken understandings about the relation of that work to ethics, scholarly method, and humanities theory . The initiative is exciting: the goal appears to be to tease out theory and method from a hermeneutic of practice. This aligns very nicely to what this article describes as postfoundational method. 

The problem, as always, is accommodating and being informed by views ranging from a focus on code-craft to the theoretically informed critique of Chun, Balsamo et al. Some efforts must be made, and ideally some intellectual levers must be found, to help bridge the gap. Andrew Prescott notes that digital humanists are well positioned to understand that knowledge is being turned into a commodity, a data steam disconnected from those who produce it and turned to commercial advantage by monopolistic corporations. In arguing for the necessity of the field, he suggests that …if humanities scholars wish to ensure that their understanding and engagement with human knowledge does not become another Californian commodity, it is essential to engage with the digital world, and not as consumers but as creators . Conversely, it seems logical to argue along with Liu that the opposite is also true: that digital humanists involved in building technologies should be aware of the critiques of the cultural, economic and political domains they might seek to retain intellectual freedom from. 

It’s important to acknowledge, though, that rather than rejecting insights produced through cultural critique, many of the projects referred to in this article are designed as active instantiations of them. Put another way, we could note that there might not be significant cause for concern: postfoundational DH method appears to function surprisingly well against issues presented by postindustrial culture. Not all DH projects are politically motivated — and we could perhaps hope that the bulk of focus remains on traditional topics like scholarly editions, concordances and archives, because this is where the discipline’s main service to the tradition lies — but the more politically motivated projects appear to revel in the knowledge that digital humanists’ felicity with the manipulation of the postindustrial system’s symbolic architecture (computer code, and the ICT discourses associated with the production and maintenance of not only that, but the system’s broader culture and politics) offers a power that should be mobilized for the common good. Their understanding of the engineered nature of the postindustrial world (their facility with the code that powers its key engines, their ability to build a server rather than be beholden to the dictates of an IT service desk, their ability to do things cost-free) gives them insight into the world of informational capitalism. The targeted success of the projects is due to the realization that [i]n the new, informational mode of development the source of productivity lies in the technology of knowledge generation, information processing, and symbolic communication . 

This isn’t hack versus yack, it’s hack then act. The projects align extremely well to the goals of more theoretically inclined humanists and evince awareness of the cultural and ideological implications of the technology industry . Indeed, rather than being anti-theoretical, the projects described above merely suggest a commitment to post (as opposed to anti) foundationalism. They indicate attempts to use the tools, paradigms, and concepts of digital technologies to help rethink the idea of instrumentality for the Humanities as a whole . They aim to use the insights gained from saturation in technical contexts for the greater good of the tradition, politicizing insights gained through postfoundational method and mobilizing them against the aspects of postindustrial capitalism that threaten (and homogenize) the broader arts and humanities community. Historian of technology Rosalind Williams might suggest they have decided that when culture is no longer an outer shell of context, but is part of the machine radically new methods of engagement and scholarly production are required . Whether production ends with the development of a schema, a data model, a website, a scholarly edition, a journal article or a monograph should matter less than the fact that production has occurred at all. 


# notes

[^1]: I would like to thank Alan Liu for reading an early version of this article, and
                Julia Flanders and the anonymous DHQ reviewers for
                their extensive and penetrating comments. All failures of interpretation and fact
                are the responsibility of the author. 
[^2]:  Postfoundationalism is also associated with postmodern
                        theology. The general epistemological stance (its approach to the validation
                        of Truth claims) in that strain is very similar to the
                        secular versions discussed in this article, but its ontological orientation
                        places it outside the scope of this article. 
[^3]:  At the time of writing the ontology was being
                        edited in preparation for formal deposit. 
[^4]:  Disclaimer: The author is a
                        member of this project. 